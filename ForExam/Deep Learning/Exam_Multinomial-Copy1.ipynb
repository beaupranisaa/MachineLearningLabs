{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Example MNIST sample (category 0)')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# Load data\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "\n",
    "### CHANGE NUMBER OF CLASSES!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# y_vect = np.zeros((len(y), NUMBER OF CLASSES))\n",
    "# and careful at y_vect[i, int(y[i])-1] = 1  if classes do not start at 0\n",
    "def convert_to_one_hot(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, int(y[i])] = 1\n",
    "    return y_vect\n",
    "\n",
    "# Convert target indices to one-hot representation\n",
    "\n",
    "y_indices = data.target\n",
    "y = convert_to_one_hot(y_indices)\n",
    "X = np.matrix(data.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "\n",
    "# Plot an example\n",
    "\n",
    "plt.imshow(np.reshape(X[0,:],(8,8)), 'gray')\n",
    "plt.title('Example MNIST sample (category %d)' % y_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-Coded Fully Connected Neural Network\n",
    "\n",
    "OK, now let's modify our code from class to work with this dataset and run 100 epochs of training.\n",
    "The main change is to use a one-hot encoding of the 10 classes at the output layer and to use\n",
    "the softmax activation function at the output. Some minor changes are required to calculate multinomial\n",
    "cross entropy loss rather than binary cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== train shape ==========\n",
      "(1078, 64) (1078, 10)\n",
      "========== test shape ==========\n",
      "(719, 64) (719, 10)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Normalize each input feature\n",
    "\n",
    "def normalize(X):\n",
    "    M = X.shape[0]\n",
    "    XX = X - np.tile(np.mean(X,0),[M,1])\n",
    "    XX = np.divide(XX, np.tile(np.std(XX,0),[M,1]))\n",
    "    return np.nan_to_num(XX, copy=True,nan=0.0)\n",
    "\n",
    "XX = normalize(X)\n",
    "\n",
    "idx = np.arange(0,M)\n",
    "\n",
    "# Partion data into training and testing dataset\n",
    "\n",
    "random.shuffle(idx)\n",
    "percent_train = .6\n",
    "m_train = int(M * percent_train)\n",
    "m_test = M - m_train\n",
    "train_idx = idx[0:m_train]\n",
    "test_idx = idx[m_train:M+1]\n",
    "X_train = XX[train_idx,:];\n",
    "X_test = XX[test_idx,:];\n",
    "\n",
    "y_train = y[train_idx];\n",
    "y_test = y[test_idx];\n",
    "y_test_indices = y_indices[test_idx]\n",
    "\n",
    "print(\"========== train shape ==========\")\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(\"========== test shape ==========\")\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a 3-layer network with sigmoid activation functions,\n",
    "# 6 units in layer 1, and 5 units in layer 2.\n",
    "\n",
    "h2 = 5\n",
    "h1 = 6\n",
    "W = [[], np.random.normal(0,0.1,[N,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "         np.random.normal(0,0.1,[h2,10])]\n",
    "b = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "         np.random.normal(0,0.1,[10,1])]\n",
    "L = len(W)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def softmax_act(z):\n",
    "    exps = np.exp(z)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def sigmoid_actder(z):\n",
    "    az = sigmoid_act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def tanh_act(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_actder(z):\n",
    "    az = act(z)\n",
    "    prod = np.multiply(az,az)\n",
    "    return (1 - prod)\n",
    "\n",
    "def relu_act(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def relu_actder(z):\n",
    "    z[z<=0] = 0\n",
    "    z[z>0] = 1\n",
    "    return z\n",
    "\n",
    "def leaky_relu_act(z):\n",
    "    return np.maximum(0.2* z,z)\n",
    "\n",
    "def leaky_relu_actder(z):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z < 0] = 0.2\n",
    "    return dz\n",
    "\n",
    "def linear_act(z):\n",
    "    return z\n",
    "    \n",
    "def linear_actder(z):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        if (l == L):\n",
    "            a = softmax_act(z)\n",
    "        else:\n",
    "            a = sigmoid_act(z)\n",
    "    return a\n",
    "\n",
    "def loss_binary(y, yhat):\n",
    "    return -((1 - y) * np.log(1 - yhat) + y * np.log(yhat))\n",
    "\n",
    "def loss_multi(y, yhat):\n",
    "    return - np.dot(y, np.log(yhat))\n",
    "\n",
    "def loss_mse(y,yhat):\n",
    "    return np.power(y-yhat,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss 2491.934603\n",
      "Epoch 0 test loss 1660.198223\n",
      "Epoch 1 train loss 2488.303665\n",
      "Epoch 1 test loss 1657.795873\n",
      "Epoch 2 train loss 2486.826354\n",
      "Epoch 2 test loss 1658.242624\n",
      "Epoch 3 train loss 2487.583547\n",
      "Epoch 3 test loss 1657.069849\n",
      "Epoch 4 train loss 2486.861573\n",
      "Epoch 4 test loss 1655.820853\n",
      "Epoch 5 train loss 2483.905043\n",
      "Epoch 5 test loss 1656.382180\n",
      "Epoch 6 train loss 2480.648881\n",
      "Epoch 6 test loss 1652.761934\n",
      "Epoch 7 train loss 2474.369328\n",
      "Epoch 7 test loss 1643.124997\n",
      "Epoch 8 train loss 2459.825875\n",
      "Epoch 8 test loss 1632.127400\n",
      "Epoch 9 train loss 2434.983522\n",
      "Epoch 9 test loss 1608.310883\n",
      "Epoch 10 train loss 2383.696296\n",
      "Epoch 10 test loss 1565.780319\n",
      "Epoch 11 train loss 2306.743910\n",
      "Epoch 11 test loss 1506.557126\n",
      "Epoch 12 train loss 2213.247875\n",
      "Epoch 12 test loss 1446.767219\n",
      "Epoch 13 train loss 2129.509871\n",
      "Epoch 13 test loss 1395.966011\n",
      "Epoch 14 train loss 2058.406770\n",
      "Epoch 14 test loss 1359.324429\n",
      "Epoch 15 train loss 2006.770491\n",
      "Epoch 15 test loss 1329.081796\n",
      "Epoch 16 train loss 1961.661380\n",
      "Epoch 16 test loss 1305.106886\n",
      "Epoch 17 train loss 1923.083083\n",
      "Epoch 17 test loss 1283.979539\n",
      "Epoch 18 train loss 1885.350670\n",
      "Epoch 18 test loss 1263.772321\n",
      "Epoch 19 train loss 1852.796073\n",
      "Epoch 19 test loss 1240.224433\n",
      "Epoch 20 train loss 1818.882214\n",
      "Epoch 20 test loss 1219.594281\n",
      "Epoch 21 train loss 1785.660075\n",
      "Epoch 21 test loss 1200.793399\n",
      "Epoch 22 train loss 1750.264257\n",
      "Epoch 22 test loss 1176.587413\n",
      "Epoch 23 train loss 1710.974485\n",
      "Epoch 23 test loss 1150.901636\n",
      "Epoch 24 train loss 1665.935797\n",
      "Epoch 24 test loss 1118.545663\n",
      "Epoch 25 train loss 1611.856179\n",
      "Epoch 25 test loss 1081.557809\n",
      "Epoch 26 train loss 1549.857781\n",
      "Epoch 26 test loss 1039.007567\n",
      "Epoch 27 train loss 1479.550431\n",
      "Epoch 27 test loss 993.836054\n",
      "Epoch 28 train loss 1410.561699\n",
      "Epoch 28 test loss 953.811201\n",
      "Epoch 29 train loss 1347.923911\n",
      "Epoch 29 test loss 920.174194\n",
      "Epoch 30 train loss 1292.202396\n",
      "Epoch 30 test loss 886.164097\n",
      "Epoch 31 train loss 1242.159483\n",
      "Epoch 31 test loss 858.664223\n",
      "Epoch 32 train loss 1197.199945\n",
      "Epoch 32 test loss 830.403221\n",
      "Epoch 33 train loss 1154.917852\n",
      "Epoch 33 test loss 809.721289\n",
      "Epoch 34 train loss 1117.051529\n",
      "Epoch 34 test loss 785.862497\n",
      "Epoch 35 train loss 1080.388945\n",
      "Epoch 35 test loss 766.787038\n",
      "Epoch 36 train loss 1045.964946\n",
      "Epoch 36 test loss 747.955009\n",
      "Epoch 37 train loss 1014.328291\n",
      "Epoch 37 test loss 730.012303\n",
      "Epoch 38 train loss 981.433049\n",
      "Epoch 38 test loss 711.657844\n",
      "Epoch 39 train loss 946.625748\n",
      "Epoch 39 test loss 688.309514\n",
      "Epoch 40 train loss 911.337786\n",
      "Epoch 40 test loss 663.535260\n",
      "Epoch 41 train loss 871.529447\n",
      "Epoch 41 test loss 634.899321\n",
      "Epoch 42 train loss 828.568063\n",
      "Epoch 42 test loss 605.885996\n",
      "Epoch 43 train loss 785.077736\n",
      "Epoch 43 test loss 577.841363\n",
      "Epoch 44 train loss 744.030424\n",
      "Epoch 44 test loss 547.005802\n",
      "Epoch 45 train loss 705.419643\n",
      "Epoch 45 test loss 524.351202\n",
      "Epoch 46 train loss 666.504632\n",
      "Epoch 46 test loss 503.079996\n",
      "Epoch 47 train loss 632.110597\n",
      "Epoch 47 test loss 476.279871\n",
      "Epoch 48 train loss 598.686451\n",
      "Epoch 48 test loss 457.811025\n",
      "Epoch 49 train loss 568.092925\n",
      "Epoch 49 test loss 438.501766\n",
      "Epoch 50 train loss 540.039818\n",
      "Epoch 50 test loss 424.175540\n",
      "Epoch 51 train loss 514.038081\n",
      "Epoch 51 test loss 409.778282\n",
      "Epoch 52 train loss 491.286912\n",
      "Epoch 52 test loss 398.238089\n",
      "Epoch 53 train loss 468.084839\n",
      "Epoch 53 test loss 384.067666\n",
      "Epoch 54 train loss 451.251119\n",
      "Epoch 54 test loss 375.956373\n",
      "Epoch 55 train loss 429.921286\n",
      "Epoch 55 test loss 366.468510\n",
      "Epoch 56 train loss 413.146972\n",
      "Epoch 56 test loss 358.219041\n",
      "Epoch 57 train loss 397.920166\n",
      "Epoch 57 test loss 350.602190\n",
      "Epoch 58 train loss 382.467011\n",
      "Epoch 58 test loss 345.012488\n",
      "Epoch 59 train loss 368.695292\n",
      "Epoch 59 test loss 339.533371\n",
      "Epoch 60 train loss 354.776070\n",
      "Epoch 60 test loss 332.551261\n",
      "Epoch 61 train loss 344.100537\n",
      "Epoch 61 test loss 325.230350\n",
      "Epoch 62 train loss 332.949191\n",
      "Epoch 62 test loss 319.759424\n",
      "Epoch 63 train loss 322.554675\n",
      "Epoch 63 test loss 314.770060\n",
      "Epoch 64 train loss 312.569825\n",
      "Epoch 64 test loss 314.588573\n",
      "Epoch 65 train loss 301.595103\n",
      "Epoch 65 test loss 312.124557\n",
      "Epoch 66 train loss 293.044471\n",
      "Epoch 66 test loss 308.491394\n",
      "Epoch 67 train loss 284.211388\n",
      "Epoch 67 test loss 305.050512\n",
      "Epoch 68 train loss 274.724278\n",
      "Epoch 68 test loss 299.921376\n",
      "Epoch 69 train loss 267.204257\n",
      "Epoch 69 test loss 299.829811\n",
      "Epoch 70 train loss 259.560932\n",
      "Epoch 70 test loss 296.639834\n",
      "Epoch 71 train loss 251.158113\n",
      "Epoch 71 test loss 297.781896\n",
      "Epoch 72 train loss 245.145842\n",
      "Epoch 72 test loss 297.127303\n",
      "Epoch 73 train loss 238.782273\n",
      "Epoch 73 test loss 295.659443\n",
      "Epoch 74 train loss 232.390826\n",
      "Epoch 74 test loss 288.813714\n",
      "Epoch 75 train loss 225.969886\n",
      "Epoch 75 test loss 288.602121\n",
      "Epoch 76 train loss 220.796192\n",
      "Epoch 76 test loss 291.062211\n",
      "Epoch 77 train loss 213.615321\n",
      "Epoch 77 test loss 287.061137\n",
      "Epoch 78 train loss 210.263817\n",
      "Epoch 78 test loss 286.315391\n",
      "Epoch 79 train loss 204.249657\n",
      "Epoch 79 test loss 287.789411\n",
      "Epoch 80 train loss 198.851896\n",
      "Epoch 80 test loss 285.501028\n",
      "Epoch 81 train loss 195.875588\n",
      "Epoch 81 test loss 285.640551\n",
      "Epoch 82 train loss 190.455943\n",
      "Epoch 82 test loss 284.316105\n",
      "Epoch 83 train loss 186.920220\n",
      "Epoch 83 test loss 283.566955\n",
      "Epoch 84 train loss 183.101401\n",
      "Epoch 84 test loss 283.666174\n",
      "Epoch 85 train loss 179.313355\n",
      "Epoch 85 test loss 280.814433\n",
      "Epoch 86 train loss 174.462037\n",
      "Epoch 86 test loss 277.611818\n",
      "Epoch 87 train loss 171.366822\n",
      "Epoch 87 test loss 280.462453\n",
      "Epoch 88 train loss 167.442609\n",
      "Epoch 88 test loss 277.125802\n",
      "Epoch 89 train loss 164.093409\n",
      "Epoch 89 test loss 279.431790\n",
      "Epoch 90 train loss 159.664814\n",
      "Epoch 90 test loss 274.559818\n",
      "Epoch 91 train loss 157.574807\n",
      "Epoch 91 test loss 273.604317\n",
      "Epoch 92 train loss 154.652217\n",
      "Epoch 92 test loss 279.321579\n",
      "Epoch 93 train loss 150.415421\n",
      "Epoch 93 test loss 276.839768\n",
      "Epoch 94 train loss 148.626756\n",
      "Epoch 94 test loss 274.944423\n",
      "Epoch 95 train loss 144.208568\n",
      "Epoch 95 test loss 276.807255\n",
      "Epoch 96 train loss 142.370872\n",
      "Epoch 96 test loss 276.613748\n",
      "Epoch 97 train loss 140.309467\n",
      "Epoch 97 test loss 277.237856\n",
      "Epoch 98 train loss 138.231238\n",
      "Epoch 98 test loss 277.205241\n",
      "Epoch 99 train loss 133.932849\n",
      "Epoch 99 test loss 275.371091\n",
      "Epoch 100 train loss 131.486659\n",
      "Epoch 100 test loss 275.957975\n",
      "Epoch 101 train loss 129.634864\n",
      "Epoch 101 test loss 277.643308\n",
      "Epoch 102 train loss 126.153533\n",
      "Epoch 102 test loss 278.061960\n",
      "Epoch 103 train loss 124.052318\n",
      "Epoch 103 test loss 273.601272\n",
      "Epoch 104 train loss 122.081030\n",
      "Epoch 104 test loss 274.936855\n",
      "Epoch 105 train loss 121.175133\n",
      "Epoch 105 test loss 274.193355\n",
      "Epoch 106 train loss 117.759455\n",
      "Epoch 106 test loss 273.703395\n",
      "Epoch 107 train loss 114.719863\n",
      "Epoch 107 test loss 271.437162\n",
      "Epoch 108 train loss 114.719247\n",
      "Epoch 108 test loss 279.660627\n",
      "Epoch 109 train loss 112.279050\n",
      "Epoch 109 test loss 274.411208\n",
      "Epoch 110 train loss 110.324161\n",
      "Epoch 110 test loss 272.662360\n",
      "Epoch 111 train loss 108.526954\n",
      "Epoch 111 test loss 274.623750\n",
      "Epoch 112 train loss 106.793689\n",
      "Epoch 112 test loss 273.569416\n",
      "Epoch 113 train loss 104.798816\n",
      "Epoch 113 test loss 270.762243\n",
      "Epoch 114 train loss 102.993011\n",
      "Epoch 114 test loss 274.199918\n",
      "Epoch 115 train loss 102.016900\n",
      "Epoch 115 test loss 267.613305\n",
      "Epoch 116 train loss 99.899021\n",
      "Epoch 116 test loss 267.341757\n",
      "Epoch 117 train loss 98.340723\n",
      "Epoch 117 test loss 268.264397\n",
      "Epoch 118 train loss 97.166468\n",
      "Epoch 118 test loss 267.440304\n",
      "Epoch 119 train loss 96.106424\n",
      "Epoch 119 test loss 270.936916\n",
      "Epoch 120 train loss 94.630619\n",
      "Epoch 120 test loss 268.065359\n",
      "Epoch 121 train loss 93.302495\n",
      "Epoch 121 test loss 271.553285\n",
      "Epoch 122 train loss 92.198297\n",
      "Epoch 122 test loss 270.306297\n",
      "Epoch 123 train loss 90.882323\n",
      "Epoch 123 test loss 270.371440\n",
      "Epoch 124 train loss 89.769629\n",
      "Epoch 124 test loss 268.407305\n",
      "Epoch 125 train loss 88.678848\n",
      "Epoch 125 test loss 270.331421\n",
      "Epoch 126 train loss 87.678877\n",
      "Epoch 126 test loss 270.018188\n",
      "Epoch 127 train loss 86.116145\n",
      "Epoch 127 test loss 268.694010\n",
      "Epoch 128 train loss 85.661690\n",
      "Epoch 128 test loss 271.099104\n",
      "Epoch 129 train loss 84.636584\n",
      "Epoch 129 test loss 270.331137\n",
      "Epoch 130 train loss 84.206145\n",
      "Epoch 130 test loss 268.085526\n",
      "Epoch 131 train loss 82.282700\n",
      "Epoch 131 test loss 267.632085\n",
      "Epoch 132 train loss 81.839161\n",
      "Epoch 132 test loss 269.077530\n",
      "Epoch 133 train loss 80.753421\n",
      "Epoch 133 test loss 267.481009\n",
      "Epoch 134 train loss 80.108893\n",
      "Epoch 134 test loss 272.207248\n",
      "Epoch 135 train loss 79.458805\n",
      "Epoch 135 test loss 270.620937\n",
      "Epoch 136 train loss 78.537744\n",
      "Epoch 136 test loss 268.502226\n",
      "Epoch 137 train loss 77.722605\n",
      "Epoch 137 test loss 270.329971\n",
      "Epoch 138 train loss 77.531381\n",
      "Epoch 138 test loss 270.477302\n",
      "Epoch 139 train loss 76.509090\n",
      "Epoch 139 test loss 271.731083\n",
      "Epoch 140 train loss 76.304550\n",
      "Epoch 140 test loss 270.977041\n",
      "Epoch 141 train loss 74.590854\n",
      "Epoch 141 test loss 271.044875\n",
      "Epoch 142 train loss 74.893703\n",
      "Epoch 142 test loss 272.669844\n",
      "Epoch 143 train loss 73.397733\n",
      "Epoch 143 test loss 268.413251\n",
      "Epoch 144 train loss 73.403964\n",
      "Epoch 144 test loss 273.785767\n",
      "Epoch 145 train loss 72.341133\n",
      "Epoch 145 test loss 270.473626\n",
      "Epoch 146 train loss 71.995405\n",
      "Epoch 146 test loss 272.272698\n",
      "Epoch 147 train loss 70.987781\n",
      "Epoch 147 test loss 269.956415\n",
      "Epoch 148 train loss 70.929892\n",
      "Epoch 148 test loss 271.669488\n",
      "Epoch 149 train loss 69.899656\n",
      "Epoch 149 test loss 271.835684\n",
      "Epoch 150 train loss 69.430096\n",
      "Epoch 150 test loss 272.240801\n",
      "Epoch 151 train loss 68.933335\n",
      "Epoch 151 test loss 270.994275\n",
      "Epoch 152 train loss 68.032252\n",
      "Epoch 152 test loss 271.612866\n",
      "Epoch 153 train loss 68.356780\n",
      "Epoch 153 test loss 275.568908\n",
      "Epoch 154 train loss 67.514846\n",
      "Epoch 154 test loss 274.597800\n",
      "Epoch 155 train loss 66.739425\n",
      "Epoch 155 test loss 270.775107\n",
      "Epoch 156 train loss 66.375605\n",
      "Epoch 156 test loss 273.015243\n",
      "Epoch 157 train loss 65.927776\n",
      "Epoch 157 test loss 271.445351\n",
      "Epoch 158 train loss 66.156749\n",
      "Epoch 158 test loss 274.147251\n",
      "Epoch 159 train loss 65.118615\n",
      "Epoch 159 test loss 272.193711\n",
      "Epoch 160 train loss 64.885102\n",
      "Epoch 160 test loss 270.445915\n",
      "Epoch 161 train loss 64.152727\n",
      "Epoch 161 test loss 270.355548\n",
      "Epoch 162 train loss 64.298825\n",
      "Epoch 162 test loss 276.789146\n",
      "Epoch 163 train loss 63.648619\n",
      "Epoch 163 test loss 272.932408\n",
      "Epoch 164 train loss 63.208035\n",
      "Epoch 164 test loss 271.575251\n",
      "Epoch 165 train loss 62.821229\n",
      "Epoch 165 test loss 272.562785\n",
      "Epoch 166 train loss 62.510782\n",
      "Epoch 166 test loss 273.253241\n",
      "Epoch 167 train loss 61.810530\n",
      "Epoch 167 test loss 272.247654\n",
      "Epoch 168 train loss 61.697810\n",
      "Epoch 168 test loss 271.630952\n",
      "Epoch 169 train loss 61.183096\n",
      "Epoch 169 test loss 273.595289\n",
      "Epoch 170 train loss 60.940668\n",
      "Epoch 170 test loss 273.756850\n",
      "Epoch 171 train loss 60.404846\n",
      "Epoch 171 test loss 271.837626\n",
      "Epoch 172 train loss 60.357877\n",
      "Epoch 172 test loss 272.522818\n",
      "Epoch 173 train loss 59.788727\n",
      "Epoch 173 test loss 272.851344\n",
      "Epoch 174 train loss 60.144695\n",
      "Epoch 174 test loss 271.329828\n",
      "Epoch 175 train loss 59.021590\n",
      "Epoch 175 test loss 269.224603\n",
      "Epoch 176 train loss 58.658447\n",
      "Epoch 176 test loss 270.054671\n",
      "Epoch 177 train loss 58.316884\n",
      "Epoch 177 test loss 272.309514\n",
      "Epoch 178 train loss 57.885866\n",
      "Epoch 178 test loss 272.643714\n",
      "Epoch 179 train loss 57.568283\n",
      "Epoch 179 test loss 270.811385\n",
      "Epoch 180 train loss 57.298082\n",
      "Epoch 180 test loss 274.814393\n",
      "Epoch 181 train loss 57.076593\n",
      "Epoch 181 test loss 272.807294\n",
      "Epoch 182 train loss 56.703163\n",
      "Epoch 182 test loss 273.067647\n",
      "Epoch 183 train loss 56.324061\n",
      "Epoch 183 test loss 272.107626\n",
      "Epoch 184 train loss 56.326238\n",
      "Epoch 184 test loss 273.574703\n",
      "Epoch 185 train loss 55.880045\n",
      "Epoch 185 test loss 272.396454\n",
      "Epoch 186 train loss 55.581589\n",
      "Epoch 186 test loss 271.843559\n",
      "Epoch 187 train loss 55.477120\n",
      "Epoch 187 test loss 275.713270\n",
      "Epoch 188 train loss 55.537072\n",
      "Epoch 188 test loss 273.791671\n",
      "Epoch 189 train loss 55.109230\n",
      "Epoch 189 test loss 273.230967\n",
      "Epoch 190 train loss 54.852672\n",
      "Epoch 190 test loss 277.227751\n",
      "Epoch 191 train loss 54.435902\n",
      "Epoch 191 test loss 275.595679\n",
      "Epoch 192 train loss 54.041399\n",
      "Epoch 192 test loss 274.232042\n",
      "Epoch 193 train loss 53.845086\n",
      "Epoch 193 test loss 277.596234\n",
      "Epoch 194 train loss 53.962789\n",
      "Epoch 194 test loss 275.327447\n",
      "Epoch 195 train loss 53.218316\n",
      "Epoch 195 test loss 274.584081\n",
      "Epoch 196 train loss 53.113069\n",
      "Epoch 196 test loss 276.435777\n",
      "Epoch 197 train loss 53.243877\n",
      "Epoch 197 test loss 275.719149\n",
      "Epoch 198 train loss 53.064083\n",
      "Epoch 198 test loss 275.216335\n",
      "Epoch 199 train loss 52.456605\n",
      "Epoch 199 test loss 275.108748\n",
      "Epoch 200 train loss 52.124444\n",
      "Epoch 200 test loss 274.812833\n",
      "Epoch 201 train loss 51.939485\n",
      "Epoch 201 test loss 275.936005\n",
      "Epoch 202 train loss 51.682218\n",
      "Epoch 202 test loss 274.497277\n",
      "Epoch 203 train loss 51.599587\n",
      "Epoch 203 test loss 280.095476\n",
      "Epoch 204 train loss 51.559522\n",
      "Epoch 204 test loss 276.296818\n",
      "Epoch 205 train loss 51.076095\n",
      "Epoch 205 test loss 275.621517\n",
      "Epoch 206 train loss 50.815557\n",
      "Epoch 206 test loss 276.025861\n",
      "Epoch 207 train loss 50.800813\n",
      "Epoch 207 test loss 280.418777\n",
      "Epoch 208 train loss 50.715882\n",
      "Epoch 208 test loss 277.725779\n",
      "Epoch 209 train loss 50.401822\n",
      "Epoch 209 test loss 276.870320\n",
      "Epoch 210 train loss 50.488681\n",
      "Epoch 210 test loss 278.330502\n",
      "Epoch 211 train loss 49.897300\n",
      "Epoch 211 test loss 277.153105\n",
      "Epoch 212 train loss 49.829494\n",
      "Epoch 212 test loss 276.869461\n",
      "Epoch 213 train loss 49.550299\n",
      "Epoch 213 test loss 276.541785\n",
      "Epoch 214 train loss 49.609011\n",
      "Epoch 214 test loss 281.563234\n",
      "Epoch 215 train loss 49.279255\n",
      "Epoch 215 test loss 278.514469\n",
      "Epoch 216 train loss 49.007635\n",
      "Epoch 216 test loss 279.129826\n",
      "Epoch 217 train loss 48.758488\n",
      "Epoch 217 test loss 278.246368\n",
      "Epoch 218 train loss 48.525023\n",
      "Epoch 218 test loss 277.631582\n",
      "Epoch 219 train loss 48.451087\n",
      "Epoch 219 test loss 280.610764\n",
      "Epoch 220 train loss 48.482051\n",
      "Epoch 220 test loss 280.449455\n",
      "Epoch 221 train loss 47.977517\n",
      "Epoch 221 test loss 277.818094\n",
      "Epoch 222 train loss 47.718583\n",
      "Epoch 222 test loss 279.935103\n",
      "Epoch 223 train loss 47.623552\n",
      "Epoch 223 test loss 278.524762\n",
      "Epoch 224 train loss 47.311599\n",
      "Epoch 224 test loss 278.275079\n",
      "Epoch 225 train loss 47.138828\n",
      "Epoch 225 test loss 279.665058\n",
      "Epoch 226 train loss 47.048551\n",
      "Epoch 226 test loss 281.866459\n",
      "Epoch 227 train loss 46.901840\n",
      "Epoch 227 test loss 279.303821\n",
      "Epoch 228 train loss 46.623975\n",
      "Epoch 228 test loss 279.352134\n",
      "Epoch 229 train loss 46.531941\n",
      "Epoch 229 test loss 279.915677\n",
      "Epoch 230 train loss 46.423640\n",
      "Epoch 230 test loss 279.519031\n",
      "Epoch 231 train loss 46.312631\n",
      "Epoch 231 test loss 280.037663\n",
      "Epoch 232 train loss 46.076745\n",
      "Epoch 232 test loss 280.238785\n",
      "Epoch 233 train loss 46.065322\n",
      "Epoch 233 test loss 282.807205\n",
      "Epoch 234 train loss 46.138943\n",
      "Epoch 234 test loss 281.310352\n",
      "Epoch 235 train loss 45.590615\n",
      "Epoch 235 test loss 280.028183\n",
      "Epoch 236 train loss 45.417352\n",
      "Epoch 236 test loss 282.002421\n",
      "Epoch 237 train loss 45.297636\n",
      "Epoch 237 test loss 280.822282\n",
      "Epoch 238 train loss 45.163220\n",
      "Epoch 238 test loss 281.725877\n",
      "Epoch 239 train loss 44.936325\n",
      "Epoch 239 test loss 281.081450\n",
      "Epoch 240 train loss 44.927174\n",
      "Epoch 240 test loss 282.183977\n",
      "Epoch 241 train loss 44.702904\n",
      "Epoch 241 test loss 281.459114\n",
      "Epoch 242 train loss 44.781112\n",
      "Epoch 242 test loss 282.868544\n",
      "Epoch 243 train loss 44.548549\n",
      "Epoch 243 test loss 282.048630\n",
      "Epoch 244 train loss 44.323171\n",
      "Epoch 244 test loss 281.674373\n",
      "Epoch 245 train loss 44.172407\n",
      "Epoch 245 test loss 282.259573\n",
      "Epoch 246 train loss 43.969510\n",
      "Epoch 246 test loss 282.180063\n",
      "Epoch 247 train loss 43.856064\n",
      "Epoch 247 test loss 282.978951\n",
      "Epoch 248 train loss 43.780399\n",
      "Epoch 248 test loss 283.247575\n",
      "Epoch 249 train loss 43.676092\n",
      "Epoch 249 test loss 282.425752\n",
      "Epoch 250 train loss 43.716574\n",
      "Epoch 250 test loss 284.795435\n",
      "Epoch 251 train loss 43.500375\n",
      "Epoch 251 test loss 283.099361\n",
      "Epoch 252 train loss 43.256829\n",
      "Epoch 252 test loss 283.568096\n",
      "Epoch 253 train loss 43.091384\n",
      "Epoch 253 test loss 283.916982\n",
      "Epoch 254 train loss 42.994042\n",
      "Epoch 254 test loss 283.205087\n",
      "Epoch 255 train loss 42.954701\n",
      "Epoch 255 test loss 285.806783\n",
      "Epoch 256 train loss 42.918802\n",
      "Epoch 256 test loss 284.888683\n",
      "Epoch 257 train loss 42.689897\n",
      "Epoch 257 test loss 284.057663\n",
      "Epoch 258 train loss 42.507906\n",
      "Epoch 258 test loss 284.862064\n",
      "Epoch 259 train loss 42.374397\n",
      "Epoch 259 test loss 284.596539\n",
      "Epoch 260 train loss 42.261992\n",
      "Epoch 260 test loss 284.289346\n",
      "Epoch 261 train loss 42.283586\n",
      "Epoch 261 test loss 286.171562\n",
      "Epoch 262 train loss 42.133863\n",
      "Epoch 262 test loss 284.690896\n",
      "Epoch 263 train loss 41.971357\n",
      "Epoch 263 test loss 286.341832\n",
      "Epoch 264 train loss 41.863949\n",
      "Epoch 264 test loss 285.452867\n",
      "Epoch 265 train loss 41.643760\n",
      "Epoch 265 test loss 285.690729\n",
      "Epoch 266 train loss 41.510050\n",
      "Epoch 266 test loss 286.609507\n",
      "Epoch 267 train loss 41.488115\n",
      "Epoch 267 test loss 286.594054\n",
      "Epoch 268 train loss 41.257626\n",
      "Epoch 268 test loss 285.481092\n",
      "Epoch 269 train loss 41.356532\n",
      "Epoch 269 test loss 287.221824\n",
      "Epoch 270 train loss 41.064114\n",
      "Epoch 270 test loss 287.200513\n",
      "Epoch 271 train loss 40.917599\n",
      "Epoch 271 test loss 288.561740\n",
      "Epoch 272 train loss 40.837846\n",
      "Epoch 272 test loss 288.563253\n",
      "Epoch 273 train loss 40.810626\n",
      "Epoch 273 test loss 288.999973\n",
      "Epoch 274 train loss 40.757373\n",
      "Epoch 274 test loss 288.477409\n",
      "Epoch 275 train loss 40.492681\n",
      "Epoch 275 test loss 288.042531\n",
      "Epoch 276 train loss 40.490454\n",
      "Epoch 276 test loss 290.452356\n",
      "Epoch 277 train loss 40.273471\n",
      "Epoch 277 test loss 289.658251\n",
      "Epoch 278 train loss 39.942069\n",
      "Epoch 278 test loss 289.346543\n",
      "Epoch 279 train loss 39.884508\n",
      "Epoch 279 test loss 291.031991\n",
      "Epoch 280 train loss 39.717880\n",
      "Epoch 280 test loss 289.992794\n",
      "Epoch 281 train loss 39.645379\n",
      "Epoch 281 test loss 290.833116\n",
      "Epoch 282 train loss 39.669711\n",
      "Epoch 282 test loss 290.701235\n",
      "Epoch 283 train loss 39.279741\n",
      "Epoch 283 test loss 290.421499\n",
      "Epoch 284 train loss 39.312474\n",
      "Epoch 284 test loss 292.077989\n",
      "Epoch 285 train loss 39.179522\n",
      "Epoch 285 test loss 292.271215\n",
      "Epoch 286 train loss 38.862062\n",
      "Epoch 286 test loss 291.756947\n",
      "Epoch 287 train loss 38.713207\n",
      "Epoch 287 test loss 292.893556\n",
      "Epoch 288 train loss 38.654706\n",
      "Epoch 288 test loss 292.307502\n",
      "Epoch 289 train loss 38.638429\n",
      "Epoch 289 test loss 293.277092\n",
      "Epoch 290 train loss 38.390905\n",
      "Epoch 290 test loss 292.375803\n",
      "Epoch 291 train loss 38.287312\n",
      "Epoch 291 test loss 292.731200\n",
      "Epoch 292 train loss 38.202640\n",
      "Epoch 292 test loss 294.157114\n",
      "Epoch 293 train loss 38.212918\n",
      "Epoch 293 test loss 292.776713\n",
      "Epoch 294 train loss 38.079514\n",
      "Epoch 294 test loss 293.973111\n",
      "Epoch 295 train loss 37.870031\n",
      "Epoch 295 test loss 295.821989\n",
      "Epoch 296 train loss 37.928198\n",
      "Epoch 296 test loss 296.468378\n",
      "Epoch 297 train loss 37.911119\n",
      "Epoch 297 test loss 294.565983\n",
      "Epoch 298 train loss 37.718906\n",
      "Epoch 298 test loss 294.392534\n",
      "Epoch 299 train loss 37.563537\n",
      "Epoch 299 test loss 294.884604\n",
      "Epoch 300 train loss 37.451666\n",
      "Epoch 300 test loss 295.419922\n",
      "Epoch 301 train loss 37.322272\n",
      "Epoch 301 test loss 294.920409\n",
      "Epoch 302 train loss 37.111746\n",
      "Epoch 302 test loss 296.837278\n",
      "Epoch 303 train loss 36.914933\n",
      "Epoch 303 test loss 295.854941\n",
      "Epoch 304 train loss 36.943740\n",
      "Epoch 304 test loss 298.489755\n",
      "Epoch 305 train loss 36.857539\n",
      "Epoch 305 test loss 296.459330\n",
      "Epoch 306 train loss 36.669304\n",
      "Epoch 306 test loss 298.155269\n",
      "Epoch 307 train loss 36.654897\n",
      "Epoch 307 test loss 298.043396\n",
      "Epoch 308 train loss 36.555452\n",
      "Epoch 308 test loss 299.195320\n",
      "Epoch 309 train loss 36.514487\n",
      "Epoch 309 test loss 298.269304\n",
      "Epoch 310 train loss 36.177483\n",
      "Epoch 310 test loss 299.183047\n",
      "Epoch 311 train loss 36.107883\n",
      "Epoch 311 test loss 300.156115\n",
      "Epoch 312 train loss 36.053840\n",
      "Epoch 312 test loss 301.808607\n",
      "Epoch 313 train loss 36.289513\n",
      "Epoch 313 test loss 300.388016\n",
      "Epoch 314 train loss 35.620692\n",
      "Epoch 314 test loss 299.799104\n",
      "Epoch 315 train loss 35.569335\n",
      "Epoch 315 test loss 302.196879\n",
      "Epoch 316 train loss 35.381394\n",
      "Epoch 316 test loss 303.239816\n",
      "Epoch 317 train loss 35.036040\n",
      "Epoch 317 test loss 304.321331\n",
      "Epoch 318 train loss 34.881939\n",
      "Epoch 318 test loss 302.839142\n",
      "Epoch 319 train loss 34.483511\n",
      "Epoch 319 test loss 304.788802\n",
      "Epoch 320 train loss 33.999459\n",
      "Epoch 320 test loss 302.716241\n",
      "Epoch 321 train loss 34.056015\n",
      "Epoch 321 test loss 303.592139\n",
      "Epoch 322 train loss 33.704723\n",
      "Epoch 322 test loss 304.874458\n",
      "Epoch 323 train loss 33.534258\n",
      "Epoch 323 test loss 304.441285\n",
      "Epoch 324 train loss 33.511277\n",
      "Epoch 324 test loss 307.013949\n",
      "Epoch 325 train loss 33.452985\n",
      "Epoch 325 test loss 306.232064\n",
      "Epoch 326 train loss 33.299748\n",
      "Epoch 326 test loss 306.974981\n",
      "Epoch 327 train loss 32.959934\n",
      "Epoch 327 test loss 306.526940\n",
      "Epoch 328 train loss 32.970962\n",
      "Epoch 328 test loss 307.558768\n",
      "Epoch 329 train loss 32.820505\n",
      "Epoch 329 test loss 306.468399\n",
      "Epoch 330 train loss 32.673501\n",
      "Epoch 330 test loss 306.410612\n",
      "Epoch 331 train loss 32.621305\n",
      "Epoch 331 test loss 307.417839\n",
      "Epoch 332 train loss 32.499116\n",
      "Epoch 332 test loss 307.188678\n",
      "Epoch 333 train loss 32.337417\n",
      "Epoch 333 test loss 306.280170\n",
      "Epoch 334 train loss 32.339671\n",
      "Epoch 334 test loss 307.567135\n",
      "Epoch 335 train loss 32.170367\n",
      "Epoch 335 test loss 307.295995\n",
      "Epoch 336 train loss 32.243097\n",
      "Epoch 336 test loss 309.697907\n",
      "Epoch 337 train loss 32.124899\n",
      "Epoch 337 test loss 309.236885\n",
      "Epoch 338 train loss 31.921159\n",
      "Epoch 338 test loss 309.296313\n",
      "Epoch 339 train loss 31.878320\n",
      "Epoch 339 test loss 309.986312\n",
      "Epoch 340 train loss 31.784645\n",
      "Epoch 340 test loss 309.559391\n",
      "Epoch 341 train loss 31.618637\n",
      "Epoch 341 test loss 308.838135\n",
      "Epoch 342 train loss 31.643035\n",
      "Epoch 342 test loss 310.795075\n",
      "Epoch 343 train loss 31.479274\n",
      "Epoch 343 test loss 309.798294\n",
      "Epoch 344 train loss 31.420613\n",
      "Epoch 344 test loss 310.378776\n",
      "Epoch 345 train loss 31.351209\n",
      "Epoch 345 test loss 311.320116\n",
      "Epoch 346 train loss 31.305855\n",
      "Epoch 346 test loss 310.451419\n",
      "Epoch 347 train loss 31.169841\n",
      "Epoch 347 test loss 310.851204\n",
      "Epoch 348 train loss 31.125470\n",
      "Epoch 348 test loss 311.061688\n",
      "Epoch 349 train loss 31.025502\n",
      "Epoch 349 test loss 310.101228\n",
      "Epoch 350 train loss 31.006673\n",
      "Epoch 350 test loss 310.951355\n",
      "Epoch 351 train loss 30.858633\n",
      "Epoch 351 test loss 310.619172\n",
      "Epoch 352 train loss 30.853424\n",
      "Epoch 352 test loss 311.976315\n",
      "Epoch 353 train loss 30.711442\n",
      "Epoch 353 test loss 311.969185\n",
      "Epoch 354 train loss 30.733160\n",
      "Epoch 354 test loss 314.064645\n",
      "Epoch 355 train loss 30.824267\n",
      "Epoch 355 test loss 313.332439\n",
      "Epoch 356 train loss 30.602042\n",
      "Epoch 356 test loss 312.654538\n",
      "Epoch 357 train loss 30.437056\n",
      "Epoch 357 test loss 312.637248\n",
      "Epoch 358 train loss 30.438891\n",
      "Epoch 358 test loss 312.031508\n",
      "Epoch 359 train loss 30.428497\n",
      "Epoch 359 test loss 314.244580\n",
      "Epoch 360 train loss 30.387010\n",
      "Epoch 360 test loss 313.618432\n",
      "Epoch 361 train loss 30.164559\n",
      "Epoch 361 test loss 312.816738\n",
      "Epoch 362 train loss 30.105493\n",
      "Epoch 362 test loss 313.324453\n",
      "Epoch 363 train loss 30.124819\n",
      "Epoch 363 test loss 313.230158\n",
      "Epoch 364 train loss 29.951702\n",
      "Epoch 364 test loss 312.585442\n",
      "Epoch 365 train loss 30.014497\n",
      "Epoch 365 test loss 315.751343\n",
      "Epoch 366 train loss 30.059151\n",
      "Epoch 366 test loss 314.514419\n",
      "Epoch 367 train loss 29.861801\n",
      "Epoch 367 test loss 314.095836\n",
      "Epoch 368 train loss 29.696408\n",
      "Epoch 368 test loss 314.166850\n",
      "Epoch 369 train loss 29.660744\n",
      "Epoch 369 test loss 315.659858\n",
      "Epoch 370 train loss 29.633155\n",
      "Epoch 370 test loss 314.201509\n",
      "Epoch 371 train loss 29.550921\n",
      "Epoch 371 test loss 315.391048\n",
      "Epoch 372 train loss 29.429989\n",
      "Epoch 372 test loss 314.484441\n",
      "Epoch 373 train loss 29.415178\n",
      "Epoch 373 test loss 316.294030\n",
      "Epoch 374 train loss 29.373511\n",
      "Epoch 374 test loss 315.016702\n",
      "Epoch 375 train loss 29.248985\n",
      "Epoch 375 test loss 315.107332\n",
      "Epoch 376 train loss 29.177110\n",
      "Epoch 376 test loss 315.070174\n",
      "Epoch 377 train loss 29.127971\n",
      "Epoch 377 test loss 316.322533\n",
      "Epoch 378 train loss 29.091691\n",
      "Epoch 378 test loss 315.353521\n",
      "Epoch 379 train loss 29.006884\n",
      "Epoch 379 test loss 316.623081\n",
      "Epoch 380 train loss 28.936203\n",
      "Epoch 380 test loss 315.083074\n",
      "Epoch 381 train loss 28.913755\n",
      "Epoch 381 test loss 317.228594\n",
      "Epoch 382 train loss 28.939636\n",
      "Epoch 382 test loss 315.462185\n",
      "Epoch 383 train loss 28.832701\n",
      "Epoch 383 test loss 315.648721\n",
      "Epoch 384 train loss 28.753164\n",
      "Epoch 384 test loss 316.344935\n",
      "Epoch 385 train loss 28.582666\n",
      "Epoch 385 test loss 315.456970\n",
      "Epoch 386 train loss 28.559311\n",
      "Epoch 386 test loss 315.865785\n",
      "Epoch 387 train loss 28.559938\n",
      "Epoch 387 test loss 316.658717\n",
      "Epoch 388 train loss 28.456237\n",
      "Epoch 388 test loss 316.136016\n",
      "Epoch 389 train loss 28.376937\n",
      "Epoch 389 test loss 317.173840\n",
      "Epoch 390 train loss 28.434877\n",
      "Epoch 390 test loss 316.466366\n",
      "Epoch 391 train loss 28.225867\n",
      "Epoch 391 test loss 316.027031\n",
      "Epoch 392 train loss 28.179699\n",
      "Epoch 392 test loss 315.692415\n",
      "Epoch 393 train loss 28.178921\n",
      "Epoch 393 test loss 316.564151\n",
      "Epoch 394 train loss 28.117055\n",
      "Epoch 394 test loss 316.459133\n",
      "Epoch 395 train loss 27.904306\n",
      "Epoch 395 test loss 316.247968\n",
      "Epoch 396 train loss 27.804566\n",
      "Epoch 396 test loss 315.708503\n",
      "Epoch 397 train loss 27.682150\n",
      "Epoch 397 test loss 315.487348\n",
      "Epoch 398 train loss 27.666341\n",
      "Epoch 398 test loss 316.089077\n",
      "Epoch 399 train loss 27.511104\n",
      "Epoch 399 test loss 316.663285\n",
      "Epoch 400 train loss 27.434691\n",
      "Epoch 400 test loss 315.216808\n",
      "Epoch 401 train loss 27.404351\n",
      "Epoch 401 test loss 316.557794\n",
      "Epoch 402 train loss 27.211237\n",
      "Epoch 402 test loss 315.695603\n",
      "Epoch 403 train loss 27.235845\n",
      "Epoch 403 test loss 315.306800\n",
      "Epoch 404 train loss 27.111251\n",
      "Epoch 404 test loss 318.346946\n",
      "Epoch 405 train loss 27.196875\n",
      "Epoch 405 test loss 315.411197\n",
      "Epoch 406 train loss 27.015689\n",
      "Epoch 406 test loss 315.485278\n",
      "Epoch 407 train loss 26.803048\n",
      "Epoch 407 test loss 314.877851\n",
      "Epoch 408 train loss 26.806664\n",
      "Epoch 408 test loss 318.774078\n",
      "Epoch 409 train loss 26.740691\n",
      "Epoch 409 test loss 316.434415\n",
      "Epoch 410 train loss 26.560185\n",
      "Epoch 410 test loss 316.321476\n",
      "Epoch 411 train loss 26.346431\n",
      "Epoch 411 test loss 314.845498\n",
      "Epoch 412 train loss 26.272906\n",
      "Epoch 412 test loss 316.042450\n",
      "Epoch 413 train loss 26.141960\n",
      "Epoch 413 test loss 315.181357\n",
      "Epoch 414 train loss 26.040191\n",
      "Epoch 414 test loss 315.974535\n",
      "Epoch 415 train loss 25.994299\n",
      "Epoch 415 test loss 315.541114\n",
      "Epoch 416 train loss 25.881422\n",
      "Epoch 416 test loss 315.366906\n",
      "Epoch 417 train loss 25.811153\n",
      "Epoch 417 test loss 316.226816\n",
      "Epoch 418 train loss 25.692900\n",
      "Epoch 418 test loss 315.642549\n",
      "Epoch 419 train loss 25.679695\n",
      "Epoch 419 test loss 316.191794\n",
      "Epoch 420 train loss 25.577819\n",
      "Epoch 420 test loss 315.650149\n",
      "Epoch 421 train loss 25.668799\n",
      "Epoch 421 test loss 317.082451\n",
      "Epoch 422 train loss 25.464706\n",
      "Epoch 422 test loss 315.814942\n",
      "Epoch 423 train loss 25.431555\n",
      "Epoch 423 test loss 316.665128\n",
      "Epoch 424 train loss 25.352854\n",
      "Epoch 424 test loss 316.734097\n",
      "Epoch 425 train loss 25.431346\n",
      "Epoch 425 test loss 317.090273\n",
      "Epoch 426 train loss 25.231119\n",
      "Epoch 426 test loss 316.762353\n",
      "Epoch 427 train loss 25.152850\n",
      "Epoch 427 test loss 317.302892\n",
      "Epoch 428 train loss 25.078509\n",
      "Epoch 428 test loss 316.629388\n",
      "Epoch 429 train loss 24.953816\n",
      "Epoch 429 test loss 317.587428\n",
      "Epoch 430 train loss 24.857196\n",
      "Epoch 430 test loss 316.855788\n",
      "Epoch 431 train loss 24.765291\n",
      "Epoch 431 test loss 317.853415\n",
      "Epoch 432 train loss 24.812252\n",
      "Epoch 432 test loss 317.188299\n",
      "Epoch 433 train loss 24.780123\n",
      "Epoch 433 test loss 317.510053\n",
      "Epoch 434 train loss 24.683133\n",
      "Epoch 434 test loss 319.151614\n",
      "Epoch 435 train loss 24.612311\n",
      "Epoch 435 test loss 317.957927\n",
      "Epoch 436 train loss 24.691753\n",
      "Epoch 436 test loss 318.634797\n",
      "Epoch 437 train loss 24.496441\n",
      "Epoch 437 test loss 318.279315\n",
      "Epoch 438 train loss 24.479525\n",
      "Epoch 438 test loss 318.739372\n",
      "Epoch 439 train loss 24.368449\n",
      "Epoch 439 test loss 318.578049\n",
      "Epoch 440 train loss 24.370904\n",
      "Epoch 440 test loss 318.327977\n",
      "Epoch 441 train loss 24.268911\n",
      "Epoch 441 test loss 320.461784\n",
      "Epoch 442 train loss 24.256272\n",
      "Epoch 442 test loss 318.715735\n",
      "Epoch 443 train loss 24.248139\n",
      "Epoch 443 test loss 319.901142\n",
      "Epoch 444 train loss 24.268843\n",
      "Epoch 444 test loss 318.260243\n",
      "Epoch 445 train loss 24.430519\n",
      "Epoch 445 test loss 319.192745\n",
      "Epoch 446 train loss 24.279282\n",
      "Epoch 446 test loss 322.763881\n",
      "Epoch 447 train loss 24.175142\n",
      "Epoch 447 test loss 321.674844\n",
      "Epoch 448 train loss 23.928960\n",
      "Epoch 448 test loss 320.683017\n",
      "Epoch 449 train loss 23.808450\n",
      "Epoch 449 test loss 320.040077\n",
      "Epoch 450 train loss 23.759740\n",
      "Epoch 450 test loss 320.244354\n",
      "Epoch 451 train loss 23.706559\n",
      "Epoch 451 test loss 320.256281\n",
      "Epoch 452 train loss 23.707976\n",
      "Epoch 452 test loss 320.782849\n",
      "Epoch 453 train loss 23.651097\n",
      "Epoch 453 test loss 320.667862\n",
      "Epoch 454 train loss 23.611864\n",
      "Epoch 454 test loss 321.707909\n",
      "Epoch 455 train loss 23.570341\n",
      "Epoch 455 test loss 321.378880\n",
      "Epoch 456 train loss 23.594730\n",
      "Epoch 456 test loss 321.796029\n",
      "Epoch 457 train loss 23.669268\n",
      "Epoch 457 test loss 321.123655\n",
      "Epoch 458 train loss 23.583916\n",
      "Epoch 458 test loss 320.964617\n",
      "Epoch 459 train loss 23.588543\n",
      "Epoch 459 test loss 321.556775\n",
      "Epoch 460 train loss 23.653643\n",
      "Epoch 460 test loss 322.923850\n",
      "Epoch 461 train loss 23.523238\n",
      "Epoch 461 test loss 323.048619\n",
      "Epoch 462 train loss 23.479750\n",
      "Epoch 462 test loss 321.761740\n",
      "Epoch 463 train loss 23.432941\n",
      "Epoch 463 test loss 322.876048\n",
      "Epoch 464 train loss 23.368177\n",
      "Epoch 464 test loss 322.232501\n",
      "Epoch 465 train loss 23.144636\n",
      "Epoch 465 test loss 323.343591\n",
      "Epoch 466 train loss 23.209104\n",
      "Epoch 466 test loss 323.155657\n",
      "Epoch 467 train loss 23.191932\n",
      "Epoch 467 test loss 323.413831\n",
      "Epoch 468 train loss 23.249222\n",
      "Epoch 468 test loss 324.217644\n",
      "Epoch 469 train loss 23.194974\n",
      "Epoch 469 test loss 323.562631\n",
      "Epoch 470 train loss 23.044385\n",
      "Epoch 470 test loss 323.553497\n",
      "Epoch 471 train loss 23.051578\n",
      "Epoch 471 test loss 323.783896\n",
      "Epoch 472 train loss 23.109182\n",
      "Epoch 472 test loss 323.861373\n",
      "Epoch 473 train loss 23.074271\n",
      "Epoch 473 test loss 323.829208\n",
      "Epoch 474 train loss 23.010426\n",
      "Epoch 474 test loss 323.179073\n",
      "Epoch 475 train loss 22.964331\n",
      "Epoch 475 test loss 325.250828\n",
      "Epoch 476 train loss 23.018168\n",
      "Epoch 476 test loss 324.145492\n",
      "Epoch 477 train loss 22.712267\n",
      "Epoch 477 test loss 324.199064\n",
      "Epoch 478 train loss 22.732777\n",
      "Epoch 478 test loss 323.424381\n",
      "Epoch 479 train loss 22.811675\n",
      "Epoch 479 test loss 324.074145\n",
      "Epoch 480 train loss 22.779454\n",
      "Epoch 480 test loss 323.952287\n",
      "Epoch 481 train loss 22.760685\n",
      "Epoch 481 test loss 324.487771\n",
      "Epoch 482 train loss 22.670060\n",
      "Epoch 482 test loss 325.042616\n",
      "Epoch 483 train loss 22.607105\n",
      "Epoch 483 test loss 326.173411\n",
      "Epoch 484 train loss 22.605788\n",
      "Epoch 484 test loss 325.387674\n",
      "Epoch 485 train loss 22.647840\n",
      "Epoch 485 test loss 325.240383\n",
      "Epoch 486 train loss 22.401056\n",
      "Epoch 486 test loss 325.225159\n",
      "Epoch 487 train loss 22.335278\n",
      "Epoch 487 test loss 326.911978\n",
      "Epoch 488 train loss 22.339051\n",
      "Epoch 488 test loss 326.013063\n",
      "Epoch 489 train loss 22.327037\n",
      "Epoch 489 test loss 325.999100\n",
      "Epoch 490 train loss 22.451852\n",
      "Epoch 490 test loss 326.633409\n",
      "Epoch 491 train loss 22.186679\n",
      "Epoch 491 test loss 326.598535\n",
      "Epoch 492 train loss 22.077210\n",
      "Epoch 492 test loss 326.970655\n",
      "Epoch 493 train loss 22.059891\n",
      "Epoch 493 test loss 326.935648\n",
      "Epoch 494 train loss 21.991904\n",
      "Epoch 494 test loss 325.970950\n",
      "Epoch 495 train loss 22.014452\n",
      "Epoch 495 test loss 327.667822\n",
      "Epoch 496 train loss 21.885420\n",
      "Epoch 496 test loss 326.606261\n",
      "Epoch 497 train loss 21.852787\n",
      "Epoch 497 test loss 327.788331\n",
      "Epoch 498 train loss 21.829271\n",
      "Epoch 498 test loss 326.066283\n",
      "Epoch 499 train loss 22.007654\n",
      "Epoch 499 test loss 327.716735\n"
     ]
    }
   ],
   "source": [
    "# Train for 100 epochs with mini-batch size 1\n",
    "\n",
    "cost_arr = [] \n",
    "cost_arr_test = []\n",
    "best_this_loss = 1e-16\n",
    "alpha = 0.01\n",
    "max_iter = 500\n",
    "iter_stop = 0\n",
    "\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    loss_this_iter_test = 0\n",
    "    order = np.random.permutation(m_train)\n",
    "    order_test = np.random.permutation(m_test)\n",
    "    for i in range(0, m_train):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = X_train[order[i],:].T\n",
    "        y_this = y_train[order[i],:]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            if (l == L):\n",
    "                a.append(softmax_act(z[l]))\n",
    "            else:\n",
    "                a.append(sigmoid_act(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "            \n",
    "        loss_this_pattern = loss_multi(y_this, a[L])\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "        \n",
    "        delta[L] = a[L] - np.matrix(y_this).T\n",
    "        for l in range(L,0,-1):\n",
    "            db[l] = delta[l].copy()\n",
    "            dW[l] = a[l-1] * delta[l].T\n",
    "            if l > 1:\n",
    "                # depends on your activation function in th at particular layer \n",
    "                # in this case all our activation functions are sigmoid \n",
    "                delta[l-1] = np.multiply(sigmoid_actder(z[l-1]), W[l] *\n",
    "                             delta[l])\n",
    "                \n",
    "        # Check delta calculation\n",
    "        \n",
    "        if False:\n",
    "            print('Target: %f' % y_this)\n",
    "            print('y_hat: %f' % a[L][0,0])\n",
    "            print(db)\n",
    "            y_pred = ff(x_this,W,b)\n",
    "            diff = 1e-3\n",
    "            W[1][10,0] = W[1][10,0] + diff\n",
    "            y_pred_db = ff(x_this,W,b)\n",
    "            L1 = loss(y_this,y_pred)\n",
    "            L2 = loss(y_this,y_pred_db)\n",
    "            db_finite_difference = (L2-L1)/diff\n",
    "            print('Original out %f, perturbed out %f' %\n",
    "                 (y_pred[0,0], y_pred_db[0,0]))\n",
    "            print('Theoretical dW %f, calculated db %f' %\n",
    "                  (dW[1][10,0], db_finite_difference[0,0]))\n",
    "        \n",
    "        for l in range(1,L+1):            \n",
    "            W[l] = W[l] - alpha * dW[l]\n",
    "            b[l] = b[l] - alpha * db[l]\n",
    "            \n",
    "        \n",
    "    for j in range(0, m_test):\n",
    "\n",
    "        # Grab the pattern order[j]\n",
    "\n",
    "        x_this_test = X_test[order_test[j],:].T\n",
    "        y_this_test = y_test[order_test[j],:]\n",
    "\n",
    "        # Feed forward step\n",
    "        a_test = [x_this_test]\n",
    "        z_test = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z_test.append(W[l].T*a_test[l-1]+b[l])\n",
    "            if (l == L):\n",
    "                a_test.append(softmax_act(z_test[l]))\n",
    "            else:\n",
    "                a_test.append(sigmoid_act(z_test[l]))\n",
    "        \n",
    "        loss_this_pattern_test = loss_multi(y_this_test,a_test[L])\n",
    "        loss_this_iter_test = loss_this_iter_test + loss_this_pattern_test\n",
    "            \n",
    "        # Backprop step. Note that derivative of multinomial cross entropy\n",
    "        # loss is the same as that of binary cross entropy loss. See\n",
    "        # https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
    "        # for a nice derivation.\n",
    "\n",
    "\n",
    "    cost_arr.append(loss_this_iter[0,0])\n",
    "    cost_arr_test.append(loss_this_iter_test[0,0])\n",
    "    \n",
    "    if loss_this_iter_test < best_this_loss:\n",
    "        w_best = copy.deepcopy(w)\n",
    "        b_best = copy.deepcopy(b)\n",
    "        iter_best = iter\n",
    "        print('Early stopping at Epoch %d' % (iter))\n",
    "        break\n",
    "    \n",
    "#     tol = 0.0001\n",
    "#     if len(cost_arr_test) > 50:\n",
    "#         if cost_arr_test[-2] - cost_arr_test[-1] < tol:\n",
    "#             iter_stop = iter\n",
    "#             print('Epoch %d train loss %f' % (iter, loss_this_iter))\n",
    "#             print('Epoch %d test loss %f' % (iter, loss_this_iter_test))\n",
    "#             print('Iter stop: ', iter_stop)\n",
    "#             break\n",
    "\n",
    "    print('Epoch %d train loss %f' % (iter, loss_this_iter))\n",
    "    print('Epoch %d test loss %f' % (iter, loss_this_iter_test))\n",
    "    iter_stop = iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9026\n"
     ]
    }
   ],
   "source": [
    "# Get test set accuracy\n",
    "\n",
    "def predict_y(W, b, X):\n",
    "    M = X.shape[0]\n",
    "    y_pred = np.zeros(M)\n",
    "    for i in range(X.shape[0]):\n",
    "        y_pred[i] = np.argmax(ff(X[i,:].T, W, b)) # + 1 IFFFF DOESNT CLASS DOES NOT START AT 0!!!!!\n",
    "    return y_pred\n",
    "\n",
    "y_test_predicted = predict_y(W, b, X_test)\n",
    "y_correct = y_test_predicted == y_test_indices\n",
    "test_accuracy = np.sum(y_correct) / len(y_correct)\n",
    "\n",
    "print('Test accuracy: %.4f' % (test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_test_predicted)\n",
    "# print(y_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,iter_stop+1,1), cost_arr, label = \"train\")\n",
    "plt.plot(np.arange(0,iter_stop+1,1), cost_arr_test, label = \"test\")\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
