{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change init W,b\n",
    "#change train: XY_act,XY_act,XY_actder, loss\n",
    "#change test: XY_act,XY_act, loss\n",
    "#change ff: XY_act,XY_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import copy\n",
    "\n",
    "data = load_breast_cancer()\n",
    "y_indices = data.target\n",
    "y = np.matrix(data.target).T\n",
    "X = np.matrix(data.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======X train =======\n",
      "(455, 30) (455, 1)\n",
      "=======X test =======\n",
      "(114, 30) (114, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Normalize each input feature\n",
    "\n",
    "def normalize(X):\n",
    "    M = X.shape[0]\n",
    "    XX = X - np.tile(np.mean(X,0),[M,1])\n",
    "    XX = np.divide(XX, np.tile(np.std(XX,0),[M,1]))\n",
    "    return np.nan_to_num(XX, copy=True,nan=0.0)\n",
    "\n",
    "XX = normalize(X)\n",
    "\n",
    "idx = np.arange(0,M)\n",
    "\n",
    "# Partion data into training and testing dataset\n",
    "\n",
    "random.shuffle(idx)\n",
    "percent_train = .8\n",
    "m_train = int(M * percent_train)\n",
    "m_test = M - m_train\n",
    "train_idx = idx[0:m_train]\n",
    "test_idx = idx[m_train:M+1]\n",
    "X_train = XX[train_idx,:];\n",
    "X_test = XX[test_idx,:];\n",
    "\n",
    "y_train = y[train_idx];\n",
    "y_test = y[test_idx];\n",
    "# y_test_indices = y_indices[test_idx]\n",
    "print('=======X train =======')\n",
    "print(X_train.shape,y_train.shape)\n",
    "print('=======X test =======')\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a 3-layer network with sigmoid activation functions,\n",
    "# 6 units in layer 1, and 5 units in layer 2.\n",
    "\n",
    "h2 = 5\n",
    "h1 = 6\n",
    "W = [[], np.random.normal(0,0.1,[N,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "         np.random.normal(0,0.1,[h2,1])]\n",
    "b = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "         np.random.normal(0,0.1,[1,1])]\n",
    "L = len(W)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def softmax_act(z):\n",
    "    exps = np.exp(z)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def sigmoid_actder(z):\n",
    "    az = sigmoid_act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def tanh_act(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_actder(z):\n",
    "    az = act(z)\n",
    "    prod = np.multiply(az,az)\n",
    "    return (1 - prod)\n",
    "\n",
    "def relu_act(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def relu_actder(z):\n",
    "    z[z<=0] = 0\n",
    "    z[z>0] = 1\n",
    "    return z\n",
    "\n",
    "def linear_act(z):\n",
    "    return z\n",
    "    \n",
    "def linear_actder(z):\n",
    "    return 1\n",
    "\n",
    "def leaky_relu_act(z):\n",
    "    return np.maximum(0.2* z,z)\n",
    "\n",
    "def leaky_relu_actder(z):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z < 0] = 0.2\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        if (l == L):\n",
    "            a = sigmoid_act(z)\n",
    "        else:\n",
    "            a = relu_act(z)\n",
    "    return a\n",
    "\n",
    "####MSE\n",
    "def loss_binary(y, yhat):\n",
    "    return -((1 - y) * np.log(1 - yhat) + y * np.log(yhat))\n",
    "\n",
    "def loss_multi(y, yhat):\n",
    "    return - np.dot(y, np.log(yhat))\n",
    "\n",
    "def loss_mse(y,yhat):\n",
    "    return np.power(y-yhat,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss 307.365650\n",
      "Epoch 0 test loss 77.504576\n",
      "Epoch 1 train loss 305.367435\n",
      "Epoch 1 test loss 77.239330\n",
      "Epoch 2 train loss 303.743636\n",
      "Epoch 2 test loss 77.034632\n",
      "Epoch 3 train loss 302.379143\n",
      "Epoch 3 test loss 76.871308\n",
      "Epoch 4 train loss 301.226556\n",
      "Epoch 4 test loss 76.738894\n",
      "Epoch 5 train loss 300.234195\n",
      "Epoch 5 test loss 76.623651\n",
      "Epoch 6 train loss 299.333592\n",
      "Epoch 6 test loss 76.511351\n",
      "Epoch 7 train loss 298.462554\n",
      "Epoch 7 test loss 76.393684\n",
      "Epoch 8 train loss 297.568459\n",
      "Epoch 8 test loss 76.258630\n",
      "Epoch 9 train loss 296.606397\n",
      "Epoch 9 test loss 76.088685\n",
      "Epoch 10 train loss 295.528316\n",
      "Epoch 10 test loss 75.872891\n",
      "Epoch 11 train loss 294.243449\n",
      "Epoch 11 test loss 75.584537\n",
      "Epoch 12 train loss 292.680975\n",
      "Epoch 12 test loss 75.199380\n",
      "Epoch 13 train loss 290.774167\n",
      "Epoch 13 test loss 74.697566\n",
      "Epoch 14 train loss 288.436129\n",
      "Epoch 14 test loss 74.076776\n",
      "Epoch 15 train loss 285.578515\n",
      "Epoch 15 test loss 73.294793\n",
      "Epoch 16 train loss 281.975987\n",
      "Epoch 16 test loss 72.276152\n",
      "Epoch 17 train loss 277.279421\n",
      "Epoch 17 test loss 70.918700\n",
      "Epoch 18 train loss 271.245668\n",
      "Epoch 18 test loss 69.193956\n",
      "Epoch 19 train loss 263.549232\n",
      "Epoch 19 test loss 67.007600\n",
      "Epoch 20 train loss 253.976146\n",
      "Epoch 20 test loss 64.325837\n",
      "Epoch 21 train loss 242.633886\n",
      "Epoch 21 test loss 61.227966\n",
      "Epoch 22 train loss 229.896894\n",
      "Epoch 22 test loss 57.837259\n",
      "Epoch 23 train loss 216.249671\n",
      "Epoch 23 test loss 54.281322\n",
      "Epoch 24 train loss 202.102306\n",
      "Epoch 24 test loss 50.581651\n",
      "Epoch 25 train loss 187.426432\n",
      "Epoch 25 test loss 46.767700\n",
      "Epoch 26 train loss 172.266367\n",
      "Epoch 26 test loss 42.808123\n",
      "Epoch 27 train loss 156.258191\n",
      "Epoch 27 test loss 38.590062\n",
      "Epoch 28 train loss 139.445605\n",
      "Epoch 28 test loss 34.214427\n",
      "Epoch 29 train loss 122.643957\n",
      "Epoch 29 test loss 29.914726\n",
      "Epoch 30 train loss 106.903249\n",
      "Epoch 30 test loss 26.007917\n",
      "Epoch 31 train loss 93.223181\n",
      "Epoch 31 test loss 22.712504\n",
      "Epoch 32 train loss 82.086082\n",
      "Epoch 32 test loss 20.056153\n",
      "Epoch 33 train loss 73.217716\n",
      "Epoch 33 test loss 17.954827\n",
      "Epoch 34 train loss 66.246643\n",
      "Epoch 34 test loss 16.307370\n",
      "Epoch 35 train loss 60.695753\n",
      "Epoch 35 test loss 15.004588\n",
      "Epoch 36 train loss 56.208837\n",
      "Epoch 36 test loss 13.965661\n",
      "Epoch 37 train loss 52.566469\n",
      "Epoch 37 test loss 13.127932\n",
      "Epoch 38 train loss 49.482438\n",
      "Epoch 38 test loss 12.437927\n",
      "Epoch 39 train loss 46.915344\n",
      "Epoch 39 test loss 11.863854\n",
      "Epoch 40 train loss 44.693323\n",
      "Epoch 40 test loss 11.371393\n",
      "Epoch 41 train loss 42.793982\n",
      "Epoch 41 test loss 10.957763\n",
      "Epoch 42 train loss 41.143124\n",
      "Epoch 42 test loss 10.609087\n",
      "Epoch 43 train loss 39.706537\n",
      "Epoch 43 test loss 10.298946\n",
      "Epoch 44 train loss 38.450142\n",
      "Epoch 44 test loss 10.029442\n",
      "Epoch 45 train loss 37.323421\n",
      "Epoch 45 test loss 9.809068\n",
      "Epoch 46 train loss 36.318175\n",
      "Epoch 46 test loss 9.621342\n",
      "Epoch 47 train loss 35.407807\n",
      "Epoch 47 test loss 9.453585\n",
      "Epoch 48 train loss 34.580474\n",
      "Epoch 48 test loss 9.317239\n",
      "Epoch 49 train loss 33.817284\n",
      "Epoch 49 test loss 9.196794\n",
      "Epoch 50 train loss 33.126081\n",
      "Epoch 50 test loss 9.083552\n",
      "Epoch 51 train loss 32.489355\n",
      "Epoch 51 test loss 8.983073\n",
      "Epoch 52 train loss 31.877902\n",
      "Epoch 52 test loss 8.904304\n",
      "Epoch 53 train loss 31.317078\n",
      "Epoch 53 test loss 8.821542\n",
      "Epoch 54 train loss 30.784074\n",
      "Epoch 54 test loss 8.755559\n",
      "Epoch 55 train loss 30.290634\n",
      "Epoch 55 test loss 8.694835\n",
      "Epoch 56 train loss 29.816374\n",
      "Epoch 56 test loss 8.635247\n",
      "Epoch 57 train loss 29.374953\n",
      "Epoch 57 test loss 8.592352\n",
      "Epoch 58 train loss 28.942748\n",
      "Epoch 58 test loss 8.563596\n",
      "Epoch 59 train loss 28.535766\n",
      "Epoch 59 test loss 8.534176\n",
      "Epoch 60 train loss 28.143223\n",
      "Epoch 60 test loss 8.524742\n",
      "Epoch 61 train loss 27.768762\n",
      "Epoch 61 test loss 8.503086\n",
      "Epoch 62 train loss 27.421198\n",
      "Epoch 62 test loss 8.499828\n",
      "Epoch 63 train loss 27.087594\n",
      "Epoch 63 test loss 8.494439\n",
      "Epoch 64 train loss 26.762494\n",
      "Epoch 64 test loss 8.496693\n",
      "Epoch 65 train loss 26.460709\n",
      "Epoch 65 test loss 8.495063\n",
      "Epoch 66 train loss 26.168387\n",
      "Epoch 66 test loss 8.487884\n",
      "Epoch 67 train loss 25.879541\n",
      "Epoch 67 test loss 8.476883\n",
      "Epoch 68 train loss 25.604364\n",
      "Epoch 68 test loss 8.482299\n",
      "Epoch 69 train loss 25.339070\n",
      "Epoch 69 test loss 8.488850\n",
      "Epoch 70 train loss 25.080457\n",
      "Epoch 70 test loss 8.489924\n",
      "Epoch 71 train loss 24.827831\n",
      "Epoch 71 test loss 8.492427\n",
      "Epoch 72 train loss 24.584188\n",
      "Epoch 72 test loss 8.515055\n",
      "Epoch 73 train loss 24.351458\n",
      "Epoch 73 test loss 8.539089\n",
      "Epoch 74 train loss 24.123163\n",
      "Epoch 74 test loss 8.548792\n",
      "Epoch 75 train loss 23.904068\n",
      "Epoch 75 test loss 8.565992\n",
      "Epoch 76 train loss 23.689828\n",
      "Epoch 76 test loss 8.598001\n",
      "Epoch 77 train loss 23.485758\n",
      "Epoch 77 test loss 8.627047\n",
      "Epoch 78 train loss 23.279214\n",
      "Epoch 78 test loss 8.677145\n",
      "Epoch 79 train loss 23.079974\n",
      "Epoch 79 test loss 8.723551\n",
      "Epoch 80 train loss 22.892575\n",
      "Epoch 80 test loss 8.748668\n",
      "Epoch 81 train loss 22.698157\n",
      "Epoch 81 test loss 8.794730\n",
      "Epoch 82 train loss 22.515436\n",
      "Epoch 82 test loss 8.835991\n",
      "Epoch 83 train loss 22.333017\n",
      "Epoch 83 test loss 8.877357\n",
      "Epoch 84 train loss 22.151212\n",
      "Epoch 84 test loss 8.934916\n",
      "Epoch 85 train loss 21.978104\n",
      "Epoch 85 test loss 8.960444\n",
      "Epoch 86 train loss 21.810910\n",
      "Epoch 86 test loss 9.001566\n",
      "Epoch 87 train loss 21.642390\n",
      "Epoch 87 test loss 9.064584\n",
      "Epoch 88 train loss 21.474148\n",
      "Epoch 88 test loss 9.101685\n",
      "Epoch 89 train loss 21.318256\n",
      "Epoch 89 test loss 9.138619\n",
      "Epoch 90 train loss 21.158010\n",
      "Epoch 90 test loss 9.180908\n",
      "Epoch 91 train loss 21.006365\n",
      "Epoch 91 test loss 9.223203\n",
      "Epoch 92 train loss 20.854326\n",
      "Epoch 92 test loss 9.282967\n",
      "Epoch 93 train loss 20.706289\n",
      "Epoch 93 test loss 9.322481\n",
      "Epoch 94 train loss 20.559605\n",
      "Epoch 94 test loss 9.380208\n",
      "Epoch 95 train loss 20.414454\n",
      "Epoch 95 test loss 9.449147\n",
      "Epoch 96 train loss 20.276013\n",
      "Epoch 96 test loss 9.495816\n",
      "Epoch 97 train loss 20.136046\n",
      "Epoch 97 test loss 9.564803\n",
      "Epoch 98 train loss 20.000425\n",
      "Epoch 98 test loss 9.655391\n",
      "Epoch 99 train loss 19.863906\n",
      "Epoch 99 test loss 9.710899\n"
     ]
    }
   ],
   "source": [
    "# Train for 100 epochs with mini-batch size 1\n",
    "\n",
    "cost_arr = [] \n",
    "cost_arr_test = []\n",
    "best_this_loss = 1e-16\n",
    "alpha = 0.001\n",
    "max_iter = 100\n",
    "iter_stop = 0\n",
    "\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    loss_this_iter_test = 0\n",
    "    order = np.random.permutation(m_train)\n",
    "    order_test = np.random.permutation(m_test)\n",
    "    for i in range(0, m_train):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = X_train[order[i],:].T\n",
    "        y_this = y_train[order[i],:]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            if (l == L):\n",
    "                a.append(sigmoid_act(z[l]))\n",
    "            else:\n",
    "                a.append(relu_act(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "            \n",
    "        loss_this_pattern = loss_binary(y_this, a[L])\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "        \n",
    "        delta[L] = a[L] - np.matrix(y_this).T\n",
    "        for l in range(L,0,-1):\n",
    "            db[l] = delta[l].copy()\n",
    "            dW[l] = a[l-1] * delta[l].T\n",
    "            if l > 1:\n",
    "                # depends on your activation function in th at particular layer \n",
    "                # in this case all our activation functions are sigmoid \n",
    "                delta[l-1] = np.multiply(relu_actder(z[l-1]), W[l] *\n",
    "                             delta[l])\n",
    "                \n",
    "        # Check delta calculation\n",
    "        \n",
    "        if False:\n",
    "            print('Target: %f' % y_this)\n",
    "            print('y_hat: %f' % a[L][0,0])\n",
    "            print(db)\n",
    "            y_pred = ff(x_this,W,b)\n",
    "            diff = 1e-3\n",
    "            W[1][10,0] = W[1][10,0] + diff\n",
    "            y_pred_db = ff(x_this,W,b)\n",
    "            L1 = loss(y_this,y_pred)\n",
    "            L2 = loss(y_this,y_pred_db)\n",
    "            db_finite_difference = (L2-L1)/diff\n",
    "            print('Original out %f, perturbed out %f' %\n",
    "                 (y_pred[0,0], y_pred_db[0,0]))\n",
    "            print('Theoretical dW %f, calculated db %f' %\n",
    "                  (dW[1][10,0], db_finite_difference[0,0]))\n",
    "        \n",
    "        for l in range(1,L+1):            \n",
    "            W[l] = W[l] - alpha * dW[l]\n",
    "            b[l] = b[l] - alpha * db[l]\n",
    "            \n",
    "        \n",
    "    for j in range(0, m_test):\n",
    "\n",
    "        # Grab the pattern order[j]\n",
    "\n",
    "        x_this_test = X_test[order_test[j],:].T\n",
    "        y_this_test = y_test[order_test[j],:]\n",
    "\n",
    "        # Feed forward step\n",
    "        a_test = [x_this_test]\n",
    "        z_test = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z_test.append(W[l].T*a_test[l-1]+b[l])\n",
    "            if (l == L):\n",
    "                a_test.append(sigmoid_act(z_test[l]))\n",
    "            else:\n",
    "                a_test.append(relu_act(z_test[l]))\n",
    "        \n",
    "        loss_this_pattern_test = loss_binary(y_this_test,a_test[L])\n",
    "        loss_this_iter_test = loss_this_iter_test + loss_this_pattern_test\n",
    "            \n",
    "        # Backprop step. Note that derivative of multinomial cross entropy\n",
    "        # loss is the same as that of binary cross entropy loss. See\n",
    "        # https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
    "        # for a nice derivation.\n",
    "\n",
    "\n",
    "    cost_arr.append(loss_this_iter[0,0])\n",
    "    cost_arr_test.append(loss_this_iter_test[0,0])\n",
    "    \n",
    "    if loss_this_iter_test < best_this_loss:\n",
    "        w_best = copy.deepcopy(w)\n",
    "        b_best = copy.deepcopy(b)\n",
    "        iter_best = iter\n",
    "        print('Early stopping at Epoch %d' % (iter))\n",
    "        break\n",
    "    \n",
    "#     tol = 0.0001\n",
    "#     if len(cost_arr_test) > 50:\n",
    "#         if cost_arr_test[-2] - cost_arr_test[-1] < tol:\n",
    "#             iter_stop = iter\n",
    "#             print('Epoch %d train loss %f' % (iter, loss_this_iter))\n",
    "#             print('Epoch %d test loss %f' % (iter, loss_this_iter_test))\n",
    "#             print('Iter stop: ', iter_stop)\n",
    "#             break\n",
    "\n",
    "    print('Epoch %d train loss %f' % (iter, loss_this_iter))\n",
    "    print('Epoch %d test loss %f' % (iter, loss_this_iter_test))\n",
    "    iter_stop = iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set accuracy\n",
    "\n",
    "# def predict_y(W, b, X):\n",
    "#     M = X.shape[0]\n",
    "#     y_pred = np.zeros(M)\n",
    "#     for i in range(X.shape[0]):\n",
    "# #         print(ff(X[i,:].T, W, b))\n",
    "#         y_pred[i] = np.argmax(ff(X[i,:].T, W, b))\n",
    "#     return y_pred\n",
    "\n",
    "def predict_y(W, b, X):\n",
    "    M = X.shape[0]\n",
    "    y_pred = np.zeros(M)\n",
    "    for i in range(X.shape[0]):\n",
    "#         print(ff(X[i,:].T, W, b))\n",
    "        y_pred[i] = ff(X[i,:].T, W, b)\n",
    "    y_pred[y_pred>0.5] = 1\n",
    "    y_pred[y_pred<0.5] = 0\n",
    "#     print(y_pred)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9649\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = predict_y(W, b, X_test)\n",
    "# print(len(y_test))\n",
    "y_test_test = np.array(y_test)\n",
    "# print('y_test',y_test_test.shape)\n",
    "y_test_predicted = np.array(y_test_predicted).reshape(-1,1)\n",
    "# print('y_test_predicted',y_test_predicted.shape)\n",
    "# print(len(y_test_predicted))\n",
    "y_correct = y_test_predicted == y_test\n",
    "test_accuracy = np.sum(y_correct) / len(y_correct)\n",
    "\n",
    "print('Test accuracy: %.4f' % (test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,iter_stop+1,1), cost_arr, label = \"train\")\n",
    "plt.plot(np.arange(0,iter_stop+1,1), cost_arr_test, label = \"test\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
