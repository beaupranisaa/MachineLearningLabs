{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct at ff, train_act,train_act,train_act_der,test_act,test_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "import copy\n",
    "\n",
    "data = load_boston()\n",
    "y_indices = data.target\n",
    "y = np.matrix(data.target).T\n",
    "X = np.matrix(data.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======X train =======\n",
      "(303, 13) (303, 1)\n",
      "=======X test =======\n",
      "(203, 13) (203, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Normalize each input feature\n",
    "\n",
    "def normalize(X):\n",
    "    M = X.shape[0]\n",
    "    XX = X - np.tile(np.mean(X,0),[M,1])\n",
    "    XX = np.divide(XX, np.tile(np.std(XX,0),[M,1]))\n",
    "    return np.nan_to_num(XX, copy=True,nan=0.0)\n",
    "\n",
    "XX = normalize(X)\n",
    "\n",
    "idx = np.arange(0,M)\n",
    "\n",
    "# Partion data into training and testing dataset\n",
    "\n",
    "random.shuffle(idx)\n",
    "percent_train = .6\n",
    "m_train = int(M * percent_train)\n",
    "m_test = M - m_train\n",
    "train_idx = idx[0:m_train]\n",
    "test_idx = idx[m_train:M+1]\n",
    "X_train = XX[train_idx,:];\n",
    "X_test = XX[test_idx,:];\n",
    "\n",
    "y_train = y[train_idx];\n",
    "y_test = y[test_idx];\n",
    "# y_test_indices = y_indices[test_idx]\n",
    "print('=======X train =======')\n",
    "print(X_train.shape,y_train.shape)\n",
    "print('=======X test =======')\n",
    "print(X_test.shape,y_test.shape)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a 3-layer network with sigmoid activation functions,\n",
    "# 6 units in layer 1, and 5 units in layer 2.\n",
    "\n",
    "h3 = 2\n",
    "h2 = 5\n",
    "h1 = 10\n",
    "\n",
    "W = [[], np.random.normal(0,0.1,[N,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "         np.random.normal(0,0.1,[h2,h3]),\n",
    "         np.random.normal(0,0.1,[h3,1])]\n",
    "b = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "         np.random.normal(0,0.1,[h3,1]),\n",
    "         np.random.normal(0,0.1,[1,1])]\n",
    "L = len(W)-1\n",
    "# print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def softmax_act(z):\n",
    "    exps = np.exp(z)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def sigmoid_actder(z):\n",
    "    az = sigmoid_act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def tanh_act(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_actder(z):\n",
    "    az = tanh_act(z)\n",
    "    prod = np.multiply(az,az)\n",
    "    return (1 - prod)\n",
    "\n",
    "def relu_act(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def relu_actder(z):\n",
    "    z[z<=0] = 0\n",
    "    z[z>0] = 1\n",
    "    return z\n",
    "\n",
    "def linear_act(z):\n",
    "    return z\n",
    "    \n",
    "def linear_actder(z):\n",
    "    return 1\n",
    "\n",
    "def leaky_relu_act(z):\n",
    "    return np.maximum(0.2* z,z)\n",
    "\n",
    "def leaky_relu_actder(z):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z < 0] = 0.2\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        if (l == L):\n",
    "            a = leaky_relu_act(z)\n",
    "        else:\n",
    "            a = leaky_relu_act(z)\n",
    "    return a\n",
    "\n",
    "####MSE\n",
    "# def loss_multi(y, yhat):\n",
    "#     return - np.dot(y, np.log(yhat))\n",
    "\n",
    "def loss(y, yhat):\n",
    "    return - np.dot(y, np.log(yhat))\n",
    "\n",
    "def loss_mse(y,yhat):\n",
    "    return np.sum(np.power((y-yhat),2))/ y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss 144075.599602\n",
      "Epoch 0 test loss 70074.741018\n",
      "Epoch 1 train loss 91398.327085\n",
      "Epoch 1 test loss 44701.481176\n",
      "Epoch 2 train loss 62522.068931\n",
      "Epoch 2 test loss 30713.696868\n",
      "Epoch 3 train loss 46565.910015\n",
      "Epoch 3 test loss 22994.616571\n",
      "Epoch 4 train loss 37408.527676\n",
      "Epoch 4 test loss 18114.021448\n",
      "Epoch 5 train loss 23930.319367\n",
      "Epoch 5 test loss 4916.884873\n",
      "Epoch 6 train loss 9394.003349\n",
      "Epoch 6 test loss 4251.851930\n",
      "Epoch 7 train loss 7546.744553\n",
      "Epoch 7 test loss 3862.974801\n",
      "Epoch 8 train loss 6290.120870\n",
      "Epoch 8 test loss 3499.293114\n",
      "Epoch 9 train loss 5680.163530\n",
      "Epoch 9 test loss 3940.494450\n",
      "Epoch 10 train loss 5124.893135\n",
      "Epoch 10 test loss 3787.217499\n",
      "Epoch 11 train loss 4924.507114\n",
      "Epoch 11 test loss 3821.579682\n",
      "Epoch 12 train loss 4355.141461\n",
      "Epoch 12 test loss 4001.563784\n",
      "Epoch 13 train loss 4607.688320\n",
      "Epoch 13 test loss 3188.192683\n",
      "Epoch 14 train loss 4329.411706\n",
      "Epoch 14 test loss 4045.511589\n",
      "Epoch 15 train loss 4057.773578\n",
      "Epoch 15 test loss 3110.104077\n",
      "Epoch 16 train loss 4071.682348\n",
      "Epoch 16 test loss 3264.348054\n",
      "Epoch 17 train loss 3804.046789\n",
      "Epoch 17 test loss 3294.489738\n",
      "Epoch 18 train loss 3621.112076\n",
      "Epoch 18 test loss 3195.254222\n",
      "Epoch 19 train loss 3520.892276\n",
      "Epoch 19 test loss 4169.487200\n",
      "Epoch 20 train loss 3468.693804\n",
      "Epoch 20 test loss 3157.753220\n",
      "Epoch 21 train loss 3463.317628\n",
      "Epoch 21 test loss 3148.224080\n",
      "Epoch 22 train loss 3474.847694\n",
      "Epoch 22 test loss 3123.331614\n",
      "Epoch 23 train loss 3511.112965\n",
      "Epoch 23 test loss 3562.171293\n",
      "Epoch 24 train loss 3380.921902\n",
      "Epoch 24 test loss 2614.032363\n",
      "Epoch 25 train loss 3329.883447\n",
      "Epoch 25 test loss 3163.315464\n",
      "Epoch 26 train loss 3181.433112\n",
      "Epoch 26 test loss 2754.867991\n",
      "Epoch 27 train loss 3158.336856\n",
      "Epoch 27 test loss 3591.962193\n",
      "Epoch 28 train loss 3042.535175\n",
      "Epoch 28 test loss 2765.380509\n",
      "Epoch 29 train loss 3102.758099\n",
      "Epoch 29 test loss 2867.638210\n",
      "Epoch 30 train loss 3029.007330\n",
      "Epoch 30 test loss 3358.175180\n",
      "Epoch 31 train loss 2986.134545\n",
      "Epoch 31 test loss 3056.555063\n",
      "Epoch 32 train loss 2997.581787\n",
      "Epoch 32 test loss 2796.023394\n",
      "Epoch 33 train loss 2835.975179\n",
      "Epoch 33 test loss 3793.165718\n",
      "Epoch 34 train loss 3070.624143\n",
      "Epoch 34 test loss 3088.441728\n",
      "Epoch 35 train loss 2933.690981\n",
      "Epoch 35 test loss 2825.747360\n",
      "Epoch 36 train loss 2885.195877\n",
      "Epoch 36 test loss 2780.237490\n",
      "Epoch 37 train loss 2887.395883\n",
      "Epoch 37 test loss 2925.457110\n",
      "Epoch 38 train loss 2744.344941\n",
      "Epoch 38 test loss 3129.949172\n",
      "Epoch 39 train loss 2740.178928\n",
      "Epoch 39 test loss 3501.143465\n",
      "Epoch 40 train loss 2746.363795\n",
      "Epoch 40 test loss 3059.934455\n",
      "Epoch 41 train loss 2725.219494\n",
      "Epoch 41 test loss 3454.839582\n",
      "Epoch 42 train loss 2803.481198\n",
      "Epoch 42 test loss 3893.424796\n",
      "Epoch 43 train loss 2688.699302\n",
      "Epoch 43 test loss 2643.591606\n",
      "Epoch 44 train loss 2560.848384\n",
      "Epoch 44 test loss 3502.217572\n",
      "Epoch 45 train loss 2752.103482\n",
      "Epoch 45 test loss 2894.051528\n",
      "Epoch 46 train loss 2622.559539\n",
      "Epoch 46 test loss 3148.017598\n",
      "Epoch 47 train loss 2842.675234\n",
      "Epoch 47 test loss 2718.482560\n",
      "Epoch 48 train loss 2704.252708\n",
      "Epoch 48 test loss 3320.128988\n",
      "Epoch 49 train loss 2713.199491\n",
      "Epoch 49 test loss 2753.020425\n",
      "Epoch 50 train loss 2476.972458\n",
      "Epoch 50 test loss 3024.062283\n",
      "Epoch 51 train loss 2561.839371\n",
      "Epoch 51 test loss 2783.848766\n",
      "Epoch 52 train loss 2704.273543\n",
      "Epoch 52 test loss 3224.172256\n",
      "Epoch 53 train loss 2568.906620\n",
      "Epoch 53 test loss 4062.755889\n",
      "Epoch 54 train loss 2649.682152\n",
      "Epoch 54 test loss 3202.227217\n",
      "Epoch 55 train loss 2455.855972\n",
      "Epoch 55 test loss 2844.226901\n",
      "Epoch 56 train loss 2560.029973\n",
      "Epoch 56 test loss 3144.707262\n",
      "Epoch 57 train loss 2555.422819\n",
      "Epoch 57 test loss 4013.909829\n",
      "Epoch 58 train loss 2594.557435\n",
      "Epoch 58 test loss 2718.721970\n",
      "Epoch 59 train loss 2552.032426\n",
      "Epoch 59 test loss 2773.262869\n",
      "Epoch 60 train loss 2568.893995\n",
      "Epoch 60 test loss 2777.186677\n",
      "Epoch 61 train loss 2394.521653\n",
      "Epoch 61 test loss 3122.927668\n",
      "Epoch 62 train loss 2433.117111\n",
      "Epoch 62 test loss 4752.404100\n",
      "Epoch 63 train loss 2560.845626\n",
      "Epoch 63 test loss 2634.654356\n",
      "Epoch 64 train loss 2429.055846\n",
      "Epoch 64 test loss 3915.983451\n",
      "Epoch 65 train loss 2476.092164\n",
      "Epoch 65 test loss 2425.112929\n",
      "Epoch 66 train loss 2415.477696\n",
      "Epoch 66 test loss 3089.904785\n",
      "Epoch 67 train loss 2318.391260\n",
      "Epoch 67 test loss 2731.076886\n",
      "Epoch 68 train loss 2317.597087\n",
      "Epoch 68 test loss 5058.827687\n",
      "Epoch 69 train loss 2319.731882\n",
      "Epoch 69 test loss 3160.183212\n",
      "Epoch 70 train loss 2557.062000\n",
      "Epoch 70 test loss 2615.271488\n",
      "Epoch 71 train loss 2509.322193\n",
      "Epoch 71 test loss 3151.334018\n",
      "Epoch 72 train loss 2365.063181\n",
      "Epoch 72 test loss 2692.712692\n",
      "Epoch 73 train loss 2272.746857\n",
      "Epoch 73 test loss 3588.652195\n",
      "Epoch 74 train loss 2477.954594\n",
      "Epoch 74 test loss 3359.002323\n",
      "Epoch 75 train loss 2415.212195\n",
      "Epoch 75 test loss 2780.704403\n",
      "Epoch 76 train loss 2354.105458\n",
      "Epoch 76 test loss 3093.256178\n",
      "Epoch 77 train loss 2334.405763\n",
      "Epoch 77 test loss 3397.181338\n",
      "Epoch 78 train loss 2377.317840\n",
      "Epoch 78 test loss 2921.842529\n",
      "Epoch 79 train loss 2339.987809\n",
      "Epoch 79 test loss 2765.298552\n",
      "Epoch 80 train loss 2334.658395\n",
      "Epoch 80 test loss 3071.277075\n",
      "Epoch 81 train loss 2334.564115\n",
      "Epoch 81 test loss 2851.627098\n",
      "Epoch 82 train loss 2346.493071\n",
      "Epoch 82 test loss 3096.246130\n",
      "Epoch 83 train loss 2360.864751\n",
      "Epoch 83 test loss 2873.484432\n",
      "Epoch 84 train loss 2288.108241\n",
      "Epoch 84 test loss 2900.852628\n",
      "Epoch 85 train loss 2205.339142\n",
      "Epoch 85 test loss 3645.643485\n",
      "Epoch 86 train loss 2336.974876\n",
      "Epoch 86 test loss 3087.662632\n",
      "Epoch 87 train loss 2219.189824\n",
      "Epoch 87 test loss 2983.624613\n",
      "Epoch 88 train loss 2215.003104\n",
      "Epoch 88 test loss 2765.242740\n",
      "Epoch 89 train loss 2231.294532\n",
      "Epoch 89 test loss 3180.024762\n",
      "Epoch 90 train loss 2288.650618\n",
      "Epoch 90 test loss 3341.046194\n",
      "Epoch 91 train loss 2255.435274\n",
      "Epoch 91 test loss 2621.918704\n",
      "Epoch 92 train loss 2193.734415\n",
      "Epoch 92 test loss 3083.673968\n",
      "Epoch 93 train loss 2109.598449\n",
      "Epoch 93 test loss 3467.311423\n",
      "Epoch 94 train loss 2044.878905\n",
      "Epoch 94 test loss 3305.321011\n",
      "Epoch 95 train loss 2093.303938\n",
      "Epoch 95 test loss 2695.435636\n",
      "Epoch 96 train loss 2150.504090\n",
      "Epoch 96 test loss 2919.998608\n",
      "Epoch 97 train loss 2125.961587\n",
      "Epoch 97 test loss 2544.031807\n",
      "Epoch 98 train loss 2197.732331\n",
      "Epoch 98 test loss 3051.976358\n",
      "Epoch 99 train loss 2024.761249\n",
      "Epoch 99 test loss 3144.530120\n"
     ]
    }
   ],
   "source": [
    "# Train for 100 epochs with mini-batch size 1\n",
    "\n",
    "cost_arr = [] \n",
    "cost_arr_test = []\n",
    "best_this_loss = 1e-16\n",
    "alpha = 0.001\n",
    "max_iter = 100\n",
    "iter_stop = 0\n",
    "\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    loss_this_iter_test = 0\n",
    "    order = np.random.permutation(m_train)\n",
    "    order_test = np.random.permutation(m_test)\n",
    "    for i in range(0, m_train):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = X_train[order[i],:].T\n",
    "        y_this = y_train[order[i],:]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        for l in range(1,L+1):\n",
    "#             print(l,W[l].shape,a[l-1].shape,b[l].shape)\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            if (l == L):\n",
    "                a.append(leaky_relu_act(z[l]))\n",
    "            else:\n",
    "                a.append(leaky_relu_act(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "            \n",
    "        loss_this_pattern = loss_mse(y_this, a[L])\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "        \n",
    "        delta[L] = a[L] - np.matrix(y_this).T\n",
    "        for l in range(L,0,-1):\n",
    "            db[l] = delta[l].copy()\n",
    "            dW[l] = a[l-1] * delta[l].T\n",
    "            if l > 1:\n",
    "                # depends on your activation function in th at particular layer \n",
    "                # in this case all our activation functions are sigmoid \n",
    "                delta[l-1] = np.multiply(leaky_relu_actder(z[l-1]), W[l] *\n",
    "                             delta[l])\n",
    "                \n",
    "        # Check delta calculation\n",
    "        \n",
    "        if False:\n",
    "            print('Target: %f' % y_this)\n",
    "            print('y_hat: %f' % a[L][0,0])\n",
    "            print(db)\n",
    "            y_pred = ff(x_this,W,b)\n",
    "            diff = 1e-3\n",
    "            W[1][10,0] = W[1][10,0] + diff\n",
    "            y_pred_db = ff(x_this,W,b)\n",
    "            L1 = loss(y_this,y_pred)\n",
    "            L2 = loss(y_this,y_pred_db)\n",
    "            db_finite_difference = (L2-L1)/diff\n",
    "            print('Original out %f, perturbed out %f' %\n",
    "                 (y_pred[0,0], y_pred_db[0,0]))\n",
    "            print('Theoretical dW %f, calculated db %f' %\n",
    "                  (dW[1][10,0], db_finite_difference[0,0]))\n",
    "        \n",
    "        for l in range(1,L+1):            \n",
    "            W[l] = W[l] - alpha * dW[l]\n",
    "            b[l] = b[l] - alpha * db[l]\n",
    "            \n",
    "        \n",
    "    for j in range(0, m_test):\n",
    "\n",
    "        # Grab the pattern order[j]\n",
    "\n",
    "        x_this_test = X_test[order_test[j],:].T\n",
    "        y_this_test = y_test[order_test[j],:]\n",
    "\n",
    "        # Feed forward step\n",
    "        a_test = [x_this_test]\n",
    "        z_test = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z_test.append(W[l].T*a_test[l-1]+b[l])\n",
    "            if (l == L):\n",
    "                a_test.append(leaky_relu_act(z_test[l]))\n",
    "            else:\n",
    "                a_test.append(leaky_relu_act(z_test[l]))\n",
    "        \n",
    "        loss_this_pattern_test = loss_mse(y_this_test,a_test[L])\n",
    "        loss_this_iter_test = loss_this_iter_test + loss_this_pattern_test\n",
    "            \n",
    "        # Backprop step. Note that derivative of multinomial cross entropy\n",
    "        # loss is the same as that of binary cross entropy loss. See\n",
    "        # https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
    "        # for a nice derivation.\n",
    "\n",
    "#     print(loss_this_iter)\n",
    "    cost_arr.append(loss_this_iter) #/m_train)\n",
    "    cost_arr_test.append(loss_this_iter_test) #/m_test)\n",
    "    \n",
    "    if loss_this_iter_test < best_this_loss:\n",
    "        w_best = copy.deepcopy(W) #w\n",
    "        b_best = copy.deepcopy(b) #b\n",
    "        iter_best = iter\n",
    "        print('Early stopping at Epoch %d' % (iter))\n",
    "        break\n",
    "    \n",
    "#     tol = 0.0001\n",
    "#     if len(cost_arr_test) > 50:\n",
    "#         if cost_arr_test[-2] - cost_arr_test[-1] < tol:\n",
    "#             iter_stop = iter\n",
    "#             print('Epoch %d train loss %f' % (iter, loss_this_iter))\n",
    "#             print('Epoch %d test loss %f' % (iter, loss_this_iter_test))\n",
    "#             print('Iter stop: ', iter_stop)\n",
    "#             break\n",
    "\n",
    "    print('Epoch %d train loss %f' % (iter, loss_this_iter))\n",
    "    print('Epoch %d test loss %f' % (iter, loss_this_iter_test))\n",
    "    iter_stop = iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.490296159326967\n",
      "0.7847145183991646\n"
     ]
    }
   ],
   "source": [
    "# Get test set accuracy\n",
    "\n",
    "def predict_y(W, b, X):\n",
    "    M = X.shape[0]\n",
    "    y_pred = np.zeros(M)\n",
    "    for i in range(X.shape[0]):\n",
    "        y_pred[i] = ff(X[i,:].T, W, b)\n",
    "    return y_pred\n",
    "\n",
    "y_test_predicted = predict_y(W, b, X_test)\n",
    "\n",
    "# y_correct = y_test_predicted == y_test\n",
    "# test_accuracy = np.sum(y_correct) / len(y_correct)\n",
    "# print('Test accuracy: %.4f' % (test_accuracy))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_test_predicted)\n",
    "r2_score = r2_score(y_test, y_test_predicted)\n",
    "print(mse)\n",
    "print(r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,iter_stop+1,1), cost_arr, label = \"train\")\n",
    "plt.plot(np.arange(0,iter_stop+1,1), cost_arr_test, label = \"test\")\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
