{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "import copy\n",
    "\n",
    "data = load_boston()\n",
    "y_indices = data.target\n",
    "y = np.matrix(data.target).T\n",
    "X = np.matrix(data.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======X train =======\n",
      "(303, 13) (303, 1)\n",
      "=======X test =======\n",
      "(203, 13) (203, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Normalize each input feature\n",
    "\n",
    "def normalize(X):\n",
    "    M = X.shape[0]\n",
    "    XX = X - np.tile(np.mean(X,0),[M,1])\n",
    "    XX = np.divide(XX, np.tile(np.std(XX,0),[M,1]))\n",
    "    return np.nan_to_num(XX, copy=True,nan=0.0)\n",
    "\n",
    "XX = normalize(X)\n",
    "\n",
    "idx = np.arange(0,M)\n",
    "\n",
    "# Partion data into training and testing dataset\n",
    "\n",
    "random.shuffle(idx)\n",
    "percent_train = .6\n",
    "m_train = int(M * percent_train)\n",
    "m_test = M - m_train\n",
    "train_idx = idx[0:m_train]\n",
    "test_idx = idx[m_train:M+1]\n",
    "X_train = XX[train_idx,:];\n",
    "X_test = XX[test_idx,:];\n",
    "\n",
    "y_train = y[train_idx];\n",
    "y_test = y[test_idx];\n",
    "# y_test_indices = y_indices[test_idx]\n",
    "print('=======X train =======')\n",
    "print(X_train.shape,y_train.shape)\n",
    "print('=======X test =======')\n",
    "print(X_test.shape,y_test.shape)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a 3-layer network with sigmoid activation functions,\n",
    "# 6 units in layer 1, and 5 units in layer 2.\n",
    "\n",
    "h3 = 2\n",
    "h2 = 5\n",
    "h1 = 10\n",
    "\n",
    "W = [[], np.random.normal(0,0.1,[N,h1]),\n",
    "         np.random.normal(0,0.1,[h1,h2]),\n",
    "         np.random.normal(0,0.1,[h2,h3]),\n",
    "         np.random.normal(0,0.1,[h3,1])]\n",
    "b = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[h2,1]),\n",
    "         np.random.normal(0,0.1,[h3,1]),\n",
    "         np.random.normal(0,0.1,[1,1])]\n",
    "L = len(W)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def softmax_act(z):\n",
    "    exps = np.exp(z)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def sigmoid_actder(z):\n",
    "    az = sigmoid_act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def tanh_act(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_actder(z):\n",
    "    az = act(z)\n",
    "    prod = np.multiply(az,az)\n",
    "    return (1 - prod)\n",
    "\n",
    "def relu_act(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def relu_actder(z):\n",
    "    z[z<=0] = 0\n",
    "    z[z>0] = 1\n",
    "    return z\n",
    "\n",
    "def linear_act(z):\n",
    "    return z\n",
    "    \n",
    "def linear_actder(z):\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        if (l == L):\n",
    "            a = relu_act(z)\n",
    "        else:\n",
    "            a = relu_act(z)\n",
    "    return a\n",
    "\n",
    "####MSE\n",
    "# def loss_multi(y, yhat):\n",
    "#     return - np.dot(y, np.log(yhat))\n",
    "\n",
    "def loss(y, yhat):\n",
    "    return - np.dot(y, np.log(yhat))\n",
    "\n",
    "def loss_mse(y,yhat):\n",
    "    return np.sum(np.power((y-yhat),2),axis=0)/ y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss 23543.733412\n",
      "Epoch 0 test loss 19274.238910\n",
      "Epoch 1 train loss 23543.171258\n",
      "Epoch 1 test loss 19270.481767\n",
      "Epoch 2 train loss 23542.825511\n",
      "Epoch 2 test loss 19267.232619\n",
      "Epoch 3 train loss 23542.147774\n",
      "Epoch 3 test loss 19267.034204\n",
      "Epoch 4 train loss 23541.417855\n",
      "Epoch 4 test loss 19259.831252\n",
      "Epoch 5 train loss 23541.654873\n",
      "Epoch 5 test loss 19263.212378\n",
      "Epoch 6 train loss 23541.771341\n",
      "Epoch 6 test loss 19264.334465\n",
      "Epoch 7 train loss 23542.689682\n",
      "Epoch 7 test loss 19262.094367\n",
      "Epoch 8 train loss 23542.390961\n",
      "Epoch 8 test loss 19260.197709\n",
      "Epoch 9 train loss 23542.303953\n",
      "Epoch 9 test loss 19257.557239\n",
      "Epoch 10 train loss 23540.933628\n",
      "Epoch 10 test loss 19252.712389\n",
      "Epoch 11 train loss 23542.368848\n",
      "Epoch 11 test loss 19255.064508\n",
      "Epoch 12 train loss 23541.081253\n",
      "Epoch 12 test loss 19259.690200\n",
      "Epoch 13 train loss 23541.843875\n",
      "Epoch 13 test loss 19255.951722\n",
      "Epoch 14 train loss 23541.382074\n",
      "Epoch 14 test loss 19253.687554\n",
      "Epoch 15 train loss 23541.689551\n",
      "Epoch 15 test loss 19251.679774\n",
      "Epoch 16 train loss 23537.036095\n",
      "Epoch 16 test loss 19241.956394\n",
      "Epoch 17 train loss 23542.485710\n",
      "Epoch 17 test loss 19242.971661\n",
      "Epoch 18 train loss 23542.246592\n",
      "Epoch 18 test loss 19242.965096\n",
      "Epoch 19 train loss 23542.553684\n",
      "Epoch 19 test loss 19245.744637\n",
      "Epoch 20 train loss 23541.873997\n",
      "Epoch 20 test loss 19250.275128\n",
      "Epoch 21 train loss 23538.625134\n",
      "Epoch 21 test loss 19243.064183\n",
      "Epoch 22 train loss 23540.182023\n",
      "Epoch 22 test loss 19253.028087\n",
      "Epoch 23 train loss 23540.615510\n",
      "Epoch 23 test loss 19246.796818\n",
      "Epoch 24 train loss 23542.616352\n",
      "Epoch 24 test loss 19248.618376\n",
      "Epoch 25 train loss 23538.441056\n",
      "Epoch 25 test loss 19241.151737\n",
      "Epoch 26 train loss 23542.031304\n",
      "Epoch 26 test loss 19239.671972\n",
      "Epoch 27 train loss 23542.650616\n",
      "Epoch 27 test loss 19245.478732\n",
      "Epoch 28 train loss 23542.258370\n",
      "Epoch 28 test loss 19245.629976\n",
      "Epoch 29 train loss 23542.327792\n",
      "Epoch 29 test loss 19245.076862\n",
      "Epoch 30 train loss 23541.723980\n",
      "Epoch 30 test loss 19250.533691\n",
      "Epoch 31 train loss 23542.373004\n",
      "Epoch 31 test loss 19251.339951\n",
      "Epoch 32 train loss 23541.733638\n",
      "Epoch 32 test loss 19254.487975\n",
      "Epoch 33 train loss 23541.896562\n",
      "Epoch 33 test loss 19255.747720\n",
      "Epoch 34 train loss 23541.917043\n",
      "Epoch 34 test loss 19259.132171\n",
      "Epoch 35 train loss 23541.234309\n",
      "Epoch 35 test loss 19262.556022\n",
      "Epoch 36 train loss 23542.437335\n",
      "Epoch 36 test loss 19258.164344\n",
      "Epoch 37 train loss 23540.477405\n",
      "Epoch 37 test loss 19263.552292\n",
      "Epoch 38 train loss 23539.201400\n",
      "Epoch 38 test loss 19252.345198\n",
      "Epoch 39 train loss 23540.735722\n",
      "Epoch 39 test loss 19259.249973\n",
      "Epoch 40 train loss 23541.431329\n",
      "Epoch 40 test loss 19255.060910\n",
      "Epoch 41 train loss 23542.057423\n",
      "Epoch 41 test loss 19257.769078\n",
      "Epoch 42 train loss 23540.326532\n",
      "Epoch 42 test loss 19264.057587\n",
      "Epoch 43 train loss 23540.741661\n",
      "Epoch 43 test loss 19254.854053\n",
      "Epoch 44 train loss 23542.351290\n",
      "Epoch 44 test loss 19256.468089\n",
      "Epoch 45 train loss 23539.919733\n",
      "Epoch 45 test loss 19248.332393\n",
      "Epoch 46 train loss 23540.650939\n",
      "Epoch 46 test loss 19245.416700\n",
      "Epoch 47 train loss 23541.838210\n",
      "Epoch 47 test loss 19243.605665\n",
      "Epoch 48 train loss 23542.164089\n",
      "Epoch 48 test loss 19244.871154\n",
      "Epoch 49 train loss 23542.437365\n",
      "Epoch 49 test loss 19247.968661\n",
      "Epoch 50 train loss 23541.641786\n",
      "Epoch 50 test loss 19252.955495\n",
      "Epoch 51 train loss 23541.866692\n",
      "Epoch 51 test loss 19249.700137\n",
      "Epoch 52 train loss 23542.092552\n",
      "Epoch 52 test loss 19251.329726\n",
      "Epoch 53 train loss 23541.442283\n",
      "Epoch 53 test loss 19248.580689\n",
      "Epoch 54 train loss 23542.403067\n",
      "Epoch 54 test loss 19247.728379\n",
      "Epoch 55 train loss 23542.228305\n",
      "Epoch 55 test loss 19252.111440\n",
      "Epoch 56 train loss 23535.297842\n",
      "Epoch 56 test loss 19265.179103\n",
      "Epoch 57 train loss 23541.733885\n",
      "Epoch 57 test loss 19266.737417\n",
      "Epoch 58 train loss 23539.687962\n",
      "Epoch 58 test loss 19271.764688\n",
      "Epoch 59 train loss 23541.909721\n",
      "Epoch 59 test loss 19265.629974\n",
      "Epoch 60 train loss 23542.325900\n",
      "Epoch 60 test loss 19265.009568\n",
      "Epoch 61 train loss 23542.200190\n",
      "Epoch 61 test loss 19258.757618\n",
      "Epoch 62 train loss 23542.242589\n",
      "Epoch 62 test loss 19257.844975\n",
      "Epoch 63 train loss 23542.228019\n",
      "Epoch 63 test loss 19258.729898\n",
      "Epoch 64 train loss 23539.163540\n",
      "Epoch 64 test loss 19265.764134\n",
      "Epoch 65 train loss 23542.309476\n",
      "Epoch 65 test loss 19259.896451\n",
      "Epoch 66 train loss 23542.233969\n",
      "Epoch 66 test loss 19258.894601\n",
      "Epoch 67 train loss 23541.625048\n",
      "Epoch 67 test loss 19260.144676\n",
      "Epoch 68 train loss 23542.482242\n",
      "Epoch 68 test loss 19259.421312\n",
      "Epoch 69 train loss 23541.910058\n",
      "Epoch 69 test loss 19258.658174\n",
      "Epoch 70 train loss 23542.151090\n",
      "Epoch 70 test loss 19259.484724\n",
      "Epoch 71 train loss 23542.212026\n",
      "Epoch 71 test loss 19257.409344\n",
      "Epoch 72 train loss 23537.811256\n",
      "Epoch 72 test loss 19247.937786\n",
      "Epoch 73 train loss 23542.211810\n",
      "Epoch 73 test loss 19249.365487\n",
      "Epoch 74 train loss 23541.628378\n",
      "Epoch 74 test loss 19248.223937\n",
      "Epoch 75 train loss 23542.265855\n",
      "Epoch 75 test loss 19251.403954\n",
      "Epoch 76 train loss 23541.770241\n",
      "Epoch 76 test loss 19253.120544\n",
      "Epoch 77 train loss 23542.343298\n",
      "Epoch 77 test loss 19252.881782\n",
      "Epoch 78 train loss 23541.942328\n",
      "Epoch 78 test loss 19250.552634\n",
      "Epoch 79 train loss 23542.425442\n",
      "Epoch 79 test loss 19252.385462\n",
      "Epoch 80 train loss 23540.638491\n",
      "Epoch 80 test loss 19259.670517\n",
      "Epoch 81 train loss 23541.780645\n",
      "Epoch 81 test loss 19259.242428\n",
      "Epoch 82 train loss 23539.284465\n",
      "Epoch 82 test loss 19250.369267\n",
      "Epoch 83 train loss 23542.352200\n",
      "Epoch 83 test loss 19252.850414\n",
      "Epoch 84 train loss 23542.076761\n",
      "Epoch 84 test loss 19250.076892\n",
      "Epoch 85 train loss 23541.021206\n",
      "Epoch 85 test loss 19257.019090\n",
      "Epoch 86 train loss 23541.056765\n",
      "Epoch 86 test loss 19251.592681\n",
      "Epoch 87 train loss 23541.955826\n",
      "Epoch 87 test loss 19253.612159\n",
      "Epoch 88 train loss 23540.776113\n",
      "Epoch 88 test loss 19247.755229\n",
      "Epoch 89 train loss 23540.719703\n",
      "Epoch 89 test loss 19255.966861\n",
      "Epoch 90 train loss 23541.874192\n",
      "Epoch 90 test loss 19256.431270\n",
      "Epoch 91 train loss 23542.516824\n",
      "Epoch 91 test loss 19254.923071\n",
      "Epoch 92 train loss 23542.338201\n",
      "Epoch 92 test loss 19255.856780\n",
      "Epoch 93 train loss 23542.278122\n",
      "Epoch 93 test loss 19253.775874\n",
      "Epoch 94 train loss 23542.179219\n",
      "Epoch 94 test loss 19252.178457\n",
      "Epoch 95 train loss 23540.865416\n",
      "Epoch 95 test loss 19246.619566\n",
      "Epoch 96 train loss 23541.595616\n",
      "Epoch 96 test loss 19244.185929\n",
      "Epoch 97 train loss 23542.357822\n",
      "Epoch 97 test loss 19245.439534\n",
      "Epoch 98 train loss 23542.405207\n",
      "Epoch 98 test loss 19246.619612\n",
      "Epoch 99 train loss 23541.758641\n",
      "Epoch 99 test loss 19244.309999\n"
     ]
    }
   ],
   "source": [
    "# Train for 100 epochs with mini-batch size 1\n",
    "\n",
    "cost_arr = [] \n",
    "cost_arr_test = []\n",
    "best_this_loss = 1e-16\n",
    "alpha = 0.001\n",
    "max_iter = 100\n",
    "iter_stop = 0\n",
    "\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    loss_this_iter_test = 0\n",
    "    order = np.random.permutation(m_train)\n",
    "    order_test = np.random.permutation(m_test)\n",
    "    for i in range(0, m_train):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = X_train[order[i],:].T\n",
    "        y_this = y_train[order[i],:]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            if (l == L):\n",
    "                a.append(relu_act(z[l]))\n",
    "            else:\n",
    "                a.append(relu_act(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "            \n",
    "        loss_this_pattern = loss_mse(y_this, a[L])\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "        \n",
    "        delta[L] = a[L] - np.matrix(y_this).T\n",
    "        for l in range(L,0,-1):\n",
    "            db[l] = delta[l].copy()\n",
    "            dW[l] = a[l-1] * delta[l].T\n",
    "            if l > 1:\n",
    "                # depends on your activation function in th at particular layer \n",
    "                # in this case all our activation functions are sigmoid \n",
    "                delta[l-1] = np.multiply(relu_actder(z[l-1]), W[l] *\n",
    "                             delta[l])\n",
    "                \n",
    "        # Check delta calculation\n",
    "        \n",
    "        if False:\n",
    "            print('Target: %f' % y_this)\n",
    "            print('y_hat: %f' % a[L][0,0])\n",
    "            print(db)\n",
    "            y_pred = ff(x_this,W,b)\n",
    "            diff = 1e-3\n",
    "            W[1][10,0] = W[1][10,0] + diff\n",
    "            y_pred_db = ff(x_this,W,b)\n",
    "            L1 = loss(y_this,y_pred)\n",
    "            L2 = loss(y_this,y_pred_db)\n",
    "            db_finite_difference = (L2-L1)/diff\n",
    "            print('Original out %f, perturbed out %f' %\n",
    "                 (y_pred[0,0], y_pred_db[0,0]))\n",
    "            print('Theoretical dW %f, calculated db %f' %\n",
    "                  (dW[1][10,0], db_finite_difference[0,0]))\n",
    "        \n",
    "        for l in range(1,L+1):            \n",
    "            W[l] = W[l] - alpha * dW[l]\n",
    "            b[l] = b[l] - alpha * db[l]\n",
    "            \n",
    "        \n",
    "    for j in range(0, m_test):\n",
    "\n",
    "        # Grab the pattern order[j]\n",
    "\n",
    "        x_this_test = X_test[order_test[j],:].T\n",
    "        y_this_test = y_test[order_test[j],:]\n",
    "\n",
    "        # Feed forward step\n",
    "        a_test = [x_this_test]\n",
    "        z_test = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z_test.append(W[l].T*a_test[l-1]+b[l])\n",
    "            if (l == L):\n",
    "                a_test.append(relu_act(z_test[l]))\n",
    "            else:\n",
    "                a_test.append(relu_act(z_test[l]))\n",
    "        \n",
    "        loss_this_pattern_test = loss_mse(y_this_test,a_test[L])\n",
    "        loss_this_iter_test = loss_this_iter_test + loss_this_pattern_test\n",
    "            \n",
    "        # Backprop step. Note that derivative of multinomial cross entropy\n",
    "        # loss is the same as that of binary cross entropy loss. See\n",
    "        # https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
    "        # for a nice derivation.\n",
    "\n",
    "\n",
    "    cost_arr.append(loss_this_iter[0,0])\n",
    "    cost_arr_test.append(loss_this_iter_test[0,0])\n",
    "    \n",
    "    if loss_this_iter_test < best_this_loss:\n",
    "        w_best = copy.deepcopy(W) #w\n",
    "        b_best = copy.deepcopy(b) #b\n",
    "        iter_best = iter\n",
    "        print('Early stopping at Epoch %d' % (iter))\n",
    "        break\n",
    "    \n",
    "#     tol = 0.0001\n",
    "#     if len(cost_arr_test) > 50:\n",
    "#         if cost_arr_test[-2] - cost_arr_test[-1] < tol:\n",
    "#             iter_stop = iter\n",
    "#             print('Epoch %d train loss %f' % (iter, loss_this_iter))\n",
    "#             print('Epoch %d test loss %f' % (iter, loss_this_iter_test))\n",
    "#             print('Iter stop: ', iter_stop)\n",
    "#             break\n",
    "\n",
    "    print('Epoch %d train loss %f' % (iter, loss_this_iter))\n",
    "    print('Epoch %d test loss %f' % (iter, loss_this_iter_test))\n",
    "    iter_stop = iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624.7123645320196\n",
      "-5.635452740741806\n"
     ]
    }
   ],
   "source": [
    "# Get test set accuracy\n",
    "\n",
    "def predict_y(W, b, X):\n",
    "    M = X.shape[0]\n",
    "    y_pred = np.zeros(M)\n",
    "    for i in range(X.shape[0]):\n",
    "        y_pred[i] = np.argmax(ff(X[i,:].T, W, b))\n",
    "    return y_pred\n",
    "\n",
    "y_test_predicted = predict_y(W, b, X_test)\n",
    "\n",
    "# y_correct = y_test_predicted == y_test\n",
    "# test_accuracy = np.sum(y_correct) / len(y_correct)\n",
    "# print('Test accuracy: %.4f' % (test_accuracy))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_test_predicted)\n",
    "r2_score = r2_score(y_test, y_test_predicted)\n",
    "print(mse)\n",
    "print(r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,iter_stop+1,1), cost_arr[0:], label = \"train\")\n",
    "plt.plot(np.arange(0,iter_stop+1,1), cost_arr_test, label = \"test\")\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
