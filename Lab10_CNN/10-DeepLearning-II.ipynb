{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Convolutional Neural Networks\n",
    "\n",
    "Today we'll experiment with CNNs. We'll start with a hand-coded CNN structure based on numpy, then we'll move to PyTorch.\n",
    "\n",
    "## Hand-coded CNN\n",
    "\n",
    "This example is based on [Ahmed Gad's tutorial](https://www.kdnuggets.com/2018/04/building-convolutional-neural-network-numpy-scratch.html).\n",
    "\n",
    "We will implement a very simple CNN in numpy. The model will have\n",
    "just three layers, a convolutional layer (conv for short), a ReLU activation layer, and max pooling. The major steps involved are as follows.\n",
    "1. Reading the input image.\n",
    "2. Preparing filters.\n",
    "3. Conv layer: Convolving each filter with the input image.\n",
    "4. ReLU layer: Applying ReLU activation function on the feature maps (output of conv layer).\n",
    "5. Max Pooling layer: Applying the pooling operation on the output of ReLU layer.\n",
    "6. Stacking the conv, ReLU, and max pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading an input image\n",
    " \n",
    "The following code reads an existing image using the SciKit-Image Python library and converts it into grayscale. You may need to `pip install scikit-image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ad7f998b07c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "import skimage.data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read image\n",
    "\n",
    "img = skimage.data.chelsea()\n",
    "print('Image dimensions:', img.shape)\n",
    "\n",
    "# Convert to grayscale\n",
    "\n",
    "img = skimage.color.rgb2gray(img)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some filters for the conv layer\n",
    "\n",
    "Recall that a conv layer uses some number of convolution (actually cross correlation) filters, usually matching the number\n",
    "of channels in the input (1 in our case since the image is grayscale). Each kernel gives us one feature map (channel) in the\n",
    "result.\n",
    "\n",
    "Let's make two 3$\\times$3 filters, using the horizontal and vertical Sobel edge filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_filters = np.zeros((2,3,3))\n",
    "l1_filters[0, :, :] = np.array([[[-1, 0, 1], \n",
    "                                 [-2, 0, 2], \n",
    "                                 [-1, 0, 1]]])\n",
    "l1_filters[1, :, :] = np.array([[[-1, -2, -1], \n",
    "                                 [ 0,  0,  0], \n",
    "                                 [ 1,  2,  1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv layer feedforward step\n",
    " \n",
    "Let's convolve the input image with our filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stride 1 cross correlation of an image and a filter. We output the valid region only\n",
    "# (no padding).\n",
    "\n",
    "def convolve(img, conv_filter):\n",
    "    stride = 1\n",
    "    padding = 0\n",
    "    filter_size = conv_filter.shape[1]\n",
    "    results_dim = ((np.array(img.shape) - np.array(conv_filter.shape) + (2*padding))/stride) + 1\n",
    "    result = np.zeros((int(results_dim[0]), int(results_dim[1])))\n",
    "    \n",
    "    for r in np.arange(0, img.shape[0] - filter_size + 1):\n",
    "        for c in np.arange(0, img.shape[1]-filter_size + 1):          \n",
    "            curr_region = img[r:r+filter_size,c:c+filter_size]\n",
    "            curr_result = curr_region * conv_filter\n",
    "            conv_sum = np.sum(curr_result)\n",
    "            result[r, c] = conv_sum\n",
    "            \n",
    "    return result       \n",
    "\n",
    "# Perform convolution with a set of filters and return the result\n",
    "\n",
    "def conv(img, conv_filters):\n",
    "    # Check shape of inputs\n",
    "    if len(img.shape) != len(conv_filters.shape) - 1: \n",
    "        raise Exception(\"Error: Number of dimensions in conv filter and image do not match.\")  \n",
    "\n",
    "    # Ensure filter depth is equal to number of channels in input\n",
    "    if len(img.shape) > 2 or len(conv_filters.shape) > 3:\n",
    "        if img.shape[-1] != conv_filters.shape[-1]:\n",
    "            raise Exception(\"Error: Number of channels in both image and filter must match.\")\n",
    "            \n",
    "    # Ensure filters are square\n",
    "    if conv_filters.shape[1] != conv_filters.shape[2]: \n",
    "        raise Exception('Error: Filter must be square (number of rows and columns must match).')\n",
    "\n",
    "    # Ensure filter dimensions are odd\n",
    "    if conv_filters.shape[1]%2==0: \n",
    "        raise Exception('Error: Filter must have an odd size (number of rows and columns must be odd).')\n",
    "\n",
    "    # Prepare output\n",
    "    feature_maps = np.zeros((img.shape[0]-conv_filters.shape[1]+1, \n",
    "                             img.shape[1]-conv_filters.shape[1]+1, \n",
    "                             conv_filters.shape[0]))\n",
    "\n",
    "    # Perform convolutions\n",
    "    for filter_num in range(conv_filters.shape[0]):\n",
    "        curr_filter = conv_filters[filter_num, :]\n",
    "        # Our convolve function only handles 2D convolutions. If the input has multiple channels, we\n",
    "        # perform the 2D convolutions for each input channel separately then add them. If the input\n",
    "        # has just a single channel, we do the convolution directly.\n",
    "        if len(curr_filter.shape) > 2:\n",
    "            conv_map = convolve(img[:, :, 0], curr_filter[:, :, 0])\n",
    "            for ch_num in range(1, curr_filter.shape[-1]):\n",
    "                conv_map = conv_map + convolve(img[:, :, ch_num], \n",
    "                                      curr_filter[:, :, ch_num])\n",
    "        else:\n",
    "            conv_map = convolve(img, curr_filter)\n",
    "        feature_maps[:, :, filter_num] = conv_map\n",
    "\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = conv(img, l1_filters)\n",
    "\n",
    "print('Convolutional feature maps shape:', features.shape)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(features[:,:,0], cmap='gray')\n",
    "ax2.imshow(features[:,:,1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, right? A couple observations:\n",
    "1. We've hard coded the values in the filters, so they are sensible to us. In a real CNN, we'd be tuning the filters to minimize loss on the training set, so we wouldn't expect such perfectly structured results.\n",
    "2. Naive implementation of 2D convolutions requires 4 nested loops, which is super slow in Python. In the code above, we've replaced the two inner loops with an element-by-element matrix multiplication for the kernel and the portion of the image applicable for the current indices into the convoution result.\n",
    "\n",
    "### In-class exercise\n",
    "\n",
    "The semi-naive implementation of the convolution function above could be sped up with the use of a fast low-level 2D convolution routine that makes the best possible use of the CPU's optimized instructions, pipelining of operations, etc. Take a look at [Laurent Perrinet's blog on 2D convolution implementations](https://laurentperrinet.github.io/sciblog/posts/2017-09-20-the-fastest-2d-convolution-in-the-world.html) and see how the two fastest implementations,\n",
    "scikit and numpy, outperform other methods and should vastly outperform the Python loop above. Reimplement the convolve() function above and compare\n",
    "the times taken by the naive and optimized versions of your convolution operation for the cat image. In your report, briefly describe the experiment and the\n",
    "results you obtained.\n",
    "\n",
    "### Pooling and relu\n",
    "\n",
    "Next, consider the feedforward pooling and ReLU operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling layer with particular size and stride\n",
    "\n",
    "def pooling(feature_map, size=2, stride=2):\n",
    "    pool_out = np.zeros((np.uint16((feature_map.shape[0]-size+1)/stride+1),\n",
    "                         np.uint16((feature_map.shape[1]-size+1)/stride+1),\n",
    "                         feature_map.shape[-1]))\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        r2 = 0\n",
    "        for r in np.arange(0,feature_map.shape[0]-size+1, stride):\n",
    "            c2 = 0\n",
    "            for c in np.arange(0, feature_map.shape[1]-size+1, stride):\n",
    "                pool_out[r2, c2, map_num] = np.max([feature_map[r:r+size,  c:c+size, map_num]])\n",
    "                c2 = c2 + 1\n",
    "            r2 = r2 +1\n",
    "    return pool_out\n",
    "\n",
    "# ReLU activation function\n",
    "\n",
    "def relu(feature_map):\n",
    "    relu_out = np.zeros(feature_map.shape)\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        for r in np.arange(0,feature_map.shape[0]):\n",
    "            for c in np.arange(0, feature_map.shape[1]):\n",
    "                relu_out[r, c, map_num] = np.max([feature_map[r, c, map_num], 0])\n",
    "    return relu_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try ReLU and pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relued_features = relu(features)\n",
    "pooled_features = pooling(relued_features)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n",
    "ax1.imshow(relued_features[:,:,0], cmap='gray')\n",
    "ax2.imshow(relued_features[:,:,1], cmap='gray')\n",
    "ax3.imshow(pooled_features[:,:,0], cmap='gray')\n",
    "ax4.imshow(pooled_features[:,:,1], cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize all of the feature maps in the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First conv layer\n",
    "\n",
    "print(\"conv layer 1...\")\n",
    "l1_feature_maps = conv(img, l1_filters)\n",
    "l1_feature_maps_relu = relu(l1_feature_maps)\n",
    "l1_feature_maps_relu_pool = pooling(l1_feature_maps_relu, 2, 2)\n",
    "\n",
    "# Second conv layer\n",
    "\n",
    "print(\"conv layer 2...\")\n",
    "l2_filters = np.random.rand(3, 5, 5, l1_feature_maps_relu_pool.shape[-1])\n",
    "l2_feature_maps = conv(l1_feature_maps_relu_pool, l2_filters)\n",
    "l2_feature_maps_relu = relu(l2_feature_maps)\n",
    "l2_feature_maps_relu_pool = pooling(l2_feature_maps_relu, 2, 2)\n",
    "\n",
    "# Third conv layer\n",
    "\n",
    "print(\"conv layer 3...\")\n",
    "l3_filters = np.random.rand(1, 7, 7, l2_feature_maps_relu_pool.shape[-1])\n",
    "l3_feature_maps = conv(l2_feature_maps_relu_pool, l3_filters)\n",
    "l3_feature_maps_relu = relu(l3_feature_maps)\n",
    "l3_feature_maps_relu_pool = pooling(l3_feature_maps_relu, 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "\n",
    "fig0, ax0 = plt.subplots(nrows=1, ncols=1)\n",
    "ax0.imshow(img).set_cmap(\"gray\")\n",
    "ax0.set_title(\"Input Image\")\n",
    "ax0.get_xaxis().set_ticks([])\n",
    "ax0.get_yaxis().set_ticks([])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "fig1, ax1 = plt.subplots(nrows=3, ncols=2)\n",
    "fig1.set_figheight(10)\n",
    "fig1.set_figwidth(10)\n",
    "ax1[0, 0].imshow(l1_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[0, 0].get_xaxis().set_ticks([])\n",
    "ax1[0, 0].get_yaxis().set_ticks([])\n",
    "ax1[0, 0].set_title(\"L1-Map1\")\n",
    "\n",
    "ax1[0, 1].imshow(l1_feature_maps[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[0, 1].get_xaxis().set_ticks([])\n",
    "ax1[0, 1].get_yaxis().set_ticks([])\n",
    "ax1[0, 1].set_title(\"L1-Map2\")\n",
    "\n",
    "ax1[1, 0].imshow(l1_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[1, 0].get_xaxis().set_ticks([])\n",
    "ax1[1, 0].get_yaxis().set_ticks([])\n",
    "ax1[1, 0].set_title(\"L1-Map1ReLU\")\n",
    "\n",
    "ax1[1, 1].imshow(l1_feature_maps_relu[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[1, 1].get_xaxis().set_ticks([])\n",
    "ax1[1, 1].get_yaxis().set_ticks([])\n",
    "ax1[1, 1].set_title(\"L1-Map2ReLU\")\n",
    "\n",
    "ax1[2, 0].imshow(l1_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax1[2, 0].get_xaxis().set_ticks([])\n",
    "ax1[2, 0].get_yaxis().set_ticks([])\n",
    "ax1[2, 0].set_title(\"L1-Map1ReLUPool\")\n",
    "\n",
    "ax1[2, 1].imshow(l1_feature_maps_relu_pool[:, :, 1]).set_cmap(\"gray\")\n",
    "ax1[2, 0].get_xaxis().set_ticks([])\n",
    "ax1[2, 0].get_yaxis().set_ticks([])\n",
    "ax1[2, 1].set_title(\"L1-Map2ReLUPool\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2\n",
    "fig2, ax2 = plt.subplots(nrows=3, ncols=3)\n",
    "fig2.set_figheight(12)\n",
    "fig2.set_figwidth(12)\n",
    "ax2[0, 0].imshow(l2_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[0, 0].get_xaxis().set_ticks([])\n",
    "ax2[0, 0].get_yaxis().set_ticks([])\n",
    "ax2[0, 0].set_title(\"L2-Map1\")\n",
    "\n",
    "ax2[0, 1].imshow(l2_feature_maps[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[0, 1].get_xaxis().set_ticks([])\n",
    "ax2[0, 1].get_yaxis().set_ticks([])\n",
    "ax2[0, 1].set_title(\"L2-Map2\")\n",
    "\n",
    "ax2[0, 2].imshow(l2_feature_maps[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[0, 2].get_xaxis().set_ticks([])\n",
    "ax2[0, 2].get_yaxis().set_ticks([])\n",
    "ax2[0, 2].set_title(\"L2-Map3\")\n",
    "\n",
    "ax2[1, 0].imshow(l2_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[1, 0].get_xaxis().set_ticks([])\n",
    "ax2[1, 0].get_yaxis().set_ticks([])\n",
    "ax2[1, 0].set_title(\"L2-Map1ReLU\")\n",
    "\n",
    "ax2[1, 1].imshow(l2_feature_maps_relu[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[1, 1].get_xaxis().set_ticks([])\n",
    "ax2[1, 1].get_yaxis().set_ticks([])\n",
    "ax2[1, 1].set_title(\"L2-Map2ReLU\")\n",
    "\n",
    "ax2[1, 2].imshow(l2_feature_maps_relu[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[1, 2].get_xaxis().set_ticks([])\n",
    "ax2[1, 2].get_yaxis().set_ticks([])\n",
    "ax2[1, 2].set_title(\"L2-Map3ReLU\")\n",
    "\n",
    "ax2[2, 0].imshow(l2_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax2[2, 0].get_xaxis().set_ticks([])\n",
    "ax2[2, 0].get_yaxis().set_ticks([])\n",
    "ax2[2, 0].set_title(\"L2-Map1ReLUPool\")\n",
    "\n",
    "ax2[2, 1].imshow(l2_feature_maps_relu_pool[:, :, 1]).set_cmap(\"gray\")\n",
    "ax2[2, 1].get_xaxis().set_ticks([])\n",
    "ax2[2, 1].get_yaxis().set_ticks([])\n",
    "ax2[2, 1].set_title(\"L2-Map2ReLUPool\")\n",
    "\n",
    "ax2[2, 2].imshow(l2_feature_maps_relu_pool[:, :, 2]).set_cmap(\"gray\")\n",
    "ax2[2, 2].get_xaxis().set_ticks([])\n",
    "ax2[2, 2].get_yaxis().set_ticks([])\n",
    "ax2[2, 2].set_title(\"L2-Map3ReLUPool\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 3\n",
    "\n",
    "fig3, ax3 = plt.subplots(nrows=1, ncols=3)\n",
    "fig3.set_figheight(15)\n",
    "fig3.set_figwidth(15)\n",
    "ax3[0].imshow(l3_feature_maps[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[0].get_xaxis().set_ticks([])\n",
    "ax3[0].get_yaxis().set_ticks([])\n",
    "ax3[0].set_title(\"L3-Map1\")\n",
    "\n",
    "ax3[1].imshow(l3_feature_maps_relu[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[1].get_xaxis().set_ticks([])\n",
    "ax3[1].get_yaxis().set_ticks([])\n",
    "ax3[1].set_title(\"L3-Map1ReLU\")\n",
    "\n",
    "ax3[2].imshow(l3_feature_maps_relu_pool[:, :, 0]).set_cmap(\"gray\")\n",
    "ax3[2].get_xaxis().set_ticks([])\n",
    "ax3[2].get_yaxis().set_ticks([])\n",
    "ax3[2].set_title(\"L3-Map1ReLUPool\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that at progressively higher layers of the network, we get coarser representations of the input. Since the filters at the later layers\n",
    "are random, they are not very structured, so we get a kind of blurring effect. These visualizations would be more meaningful in model with learned\n",
    "filters.\n",
    "\n",
    "## CNNs in PyTorch\n",
    "\n",
    "Now we'll do a more complete CNN example using PyTorch. We'll use the MNIST digits again. The example is based on\n",
    "[Anand Saha's PyTorch tutorial](https://github.com/anandsaha/deep.learning.with.pytorch).\n",
    "\n",
    "PyTorch has a few useful modules for us:\n",
    "1. cuda: GPU-based tensor computations\n",
    "2. nn: Neural network layer implementations and backpropagation via autograd\n",
    "3. torchvision: datasets, models, and image transformations for computer vision problems.\n",
    "\n",
    "torchvision itself includes several useful elements:\n",
    "1. datasets: Datasets are subclasses of torch.utils.data.Dataset. Some of the common datasets available are \"MNIST,\" \"COCO,\" and \"CIFAR.\" In this example we will see how to load MNIST dataset using a custom subclass of the datasets class.\n",
    "2. transforms - Transforms are used for image transformations. The MNIST dataset from torchvision is in PIL image. \n",
    "To convert MNIST images to tensors, we will use `transforms.ToTensor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "# The functional module contains helper functions for defining neural network layers as simple functions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST data\n",
    "\n",
    "First, let's load the data and transfrom the input elements (pixels) so that their mean over the entire training dataset is 0 and its standard deviation is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired mean and standard deviation\n",
    "\n",
    "mean = 0.0\n",
    "stddev = 1.0\n",
    "\n",
    "# Transform input image\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((mean), (stddev))])\n",
    "\n",
    "mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "mnist_valid = datasets.MNIST('./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mnist_train[12][0].numpy()\n",
    "plt.imshow(img.reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = mnist_train[12][1]\n",
    "print('Label of image above:', label)\n",
    "\n",
    "# Reduce batch size if you get out-of-memory error\n",
    "\n",
    "batch_size = 1024\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "mnist_valid_loader = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the NN model\n",
    "\n",
    "We use 2 convolutional layers followed by 2 fully connected layers. The input size of each image is (28,28,1). We will use stide of size 1 and padding of size 0.\n",
    "\n",
    "For first convolution layer we will apply 20 filters of size (5,5).   CNN output formula $$\\text{output size} = \\frac{W - F + 2P}{S} + 1$$ where $W$ - input, $F$ - filter size, $P$ - padding size and $S$ - stride size.\n",
    "\n",
    "We get $\\frac{(28,28,1) - (5,5,1) + (2*0)}{1} + 1$ for each filter, so for 10 filters we get output size of (24,24,10).\n",
    "\n",
    "The ReLU activation function is applied to the output of the first convolutional layer.\n",
    "\n",
    "For the second convolutional layer, we apply 20 filters of size (5,5), giving us output of size of (20,20,20). Maxpooling with a size of 2 is applied to the output of the second convolutional layer, thereby giving us an output size of of (10,10,20). The ReLU activation function is applied to the output of the maxpooling layer.\n",
    "\n",
    "Next we have two fully connected layers. The input of the first fully connected layer is flattened output of $10*10*20 = 2000$, with 50 nodes. The second layer is the output layer and has 10 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "               \n",
    "        # NOTE: All Conv2d layers have a default padding of 0 and stride of 1,\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)      # 24 x 24 x 20  (after 1st convolution)\n",
    "        self.relu1 = nn.ReLU()                            # Same as above\n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)     # 20 x 20 x 20  (after 2nd convolution)\n",
    "        #self.conv2_drop = nn.Dropout2d(p=0.5)            # Dropout is a regularization technqiue we discussed in class\n",
    "        self.maxpool2 = nn.MaxPool2d(2)                   # 10 x 10 x 20  (after pooling)\n",
    "        self.relu2 = nn.ReLU()                            # Same as above \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(2000, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Convolution Layer 1                    \n",
    "        x = self.conv1(x)                        \n",
    "        x = self.relu1(x)                        \n",
    "        \n",
    "        # Convolution Layer 2\n",
    "        x = self.conv2(x)               \n",
    "        #x = self.conv2_drop(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Switch from activation maps to vectors\n",
    "        x = x.view(-1, 2000)\n",
    "        \n",
    "        # Fully connected layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x, training=True)\n",
    "        \n",
    "        # Fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "net = CNN_Model()\n",
    "\n",
    "if cuda.is_available():\n",
    "    net = net.cuda()\n",
    "\n",
    "# Our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Our optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ############################\n",
    "    # Train\n",
    "    ############################\n",
    "    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    net.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
    "        outputs = net(items)      # Do the forward pass\n",
    "        loss = criterion(outputs, classes) # Calculate the loss\n",
    "        iter_loss += loss.item() # Accumulate the loss\n",
    "        loss.backward()           # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append((100 * correct / len(mnist_train_loader.dataset)))\n",
    "   \n",
    "\n",
    "    ############################\n",
    "    # Validate - How did we do on the unseen dataset?\n",
    "    ############################\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    net.eval()                    # Put the network into evaluate mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(mnist_valid_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        outputs = net(items)      # Do the forward pass\n",
    "        loss += criterion(outputs, classes).item() # Calculate the loss\n",
    "        \n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the validation loss\n",
    "    valid_loss.append(loss/iterations)\n",
    "    # Record the validation accuracy\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    valid_accuracy.append(correct_scalar / len(mnist_valid_loader.dataset) * 100.0)\n",
    "\n",
    "    print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
    "           %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             valid_loss[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is still learning something. We might want to train another 10 epochs or so to see if validation accuracy increases further. For now, though, we'll just save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(net.state_dict(), \"./3.model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's visualize the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_loss, label='training loss')\n",
    "plt.plot(valid_loss, label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy curves\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_accuracy, label='training accuracy')\n",
    "plt.plot(valid_accuracy, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you conclude from the loss and accuracy curves?\n",
    "1. We are not overfitting (at least not yet)\n",
    "2. We should continue training, as validation loss is still improving\n",
    "3. Validation accuracy is much higher than last week's fully connected models\n",
    "\n",
    "Now let's test on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 9\n",
    "img = mnist_valid[image_index][0].resize_((1, 1, 28, 28))\n",
    "img = Variable(img)\n",
    "label = mnist_valid[image_index][1]\n",
    "plt.imshow(img[0,0])\n",
    "net.eval()\n",
    "\n",
    "if cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    img = img.cuda()\n",
    "else:\n",
    "    net = net.cpu()\n",
    "    img = img.cpu()\n",
    "    \n",
    "output = net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(output.data, 1)\n",
    "print(\"Predicted label:\", predicted[0].item())\n",
    "print(\"Actual label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home exercise\n",
    "\n",
    "Apply the tech you've learned up till now to take Kaggle's 2013 [Dogs vs. Cats Challenge](https://www.kaggle.com/c/dogs-vs-cats). Download the training and test datasets and try to build the best PyTorch CNN you can for this dataset. Describe your efforts and the results in a brief lab report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
